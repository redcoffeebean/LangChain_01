import os
import io
import tempfile
from pathlib import Path
from typing import List

import streamlit as st
from loguru import logger

# =========================
# LangChain ê´€ë ¨ ì„í¬íŠ¸
# =========================
# (ì¤‘ìš”) ChatOpenAIëŠ” langchain_openaiì—ì„œ ì œê³µ
from langchain_openai import ChatOpenAI

# (ì¤‘ìš”) í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”©ì€ langchain_huggingfaceì—ì„œ ì œê³µ
#  - ì˜ˆì „ì˜ langchain_community.embeddings.HuggingFaceEmbeddingsëŠ” 0.2.2ì—ì„œ Deprecated
from langchain_huggingface import HuggingFaceEmbeddings

# ì»¤ë®¤ë‹ˆí‹° ë²¡í„°ìŠ¤í† ì–´/ë¡œë”
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredPowerPointLoader,
)

# í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°(ë³„ë„ íŒ¨í‚¤ì§€)
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ì²´ì¸/ë©”ëª¨ë¦¬
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory


# =========================
# Streamlit ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="RAG Chatbot (OpenAI + HF)", page_icon="ğŸ¤–")
st.title("RAG Chatbot âœ¨")
# st.text("RAG (Retrieval Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” LLMì´ ì§€ë‹ˆëŠ” í†µì œí•˜ê¸° ì–´ë ¤ìš´ í•œê³„, ì¦‰ â€˜ì‚¬ì‹¤ ê´€ê³„ ì˜¤ë¥˜ ê°€ëŠ¥ì„±â€™ (Hallucination)ê³¼ â€˜ë§¥ë½ ì´í•´ì˜ í•œê³„â€™ë¥¼ ê°œì„ í•˜ê³  ë³´ì™„í•˜ëŠ” ë° ì´ˆì ì„ ë‘” ë°©ë²•ì…ë‹ˆë‹¤. RAGëŠ” LLMì— ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬, ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì€ ë¬¼ë¡  íŠ¹ì • ì‚°ì—… ë„ë©”ì¸ì— íŠ¹í™”ëœ ì •ë³´ì™€ ìµœì‹  ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‚¬ì‹¤ ê´€ê³„ íŒŒì•… ëŠ¥ë ¥ê¹Œì§€ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.")


# =========================
# ìœ í‹¸/í—¬í¼
# =========================
def _persist_upload(file) -> Path:
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ í´ë”ì— ì €ì¥ í›„ ê²½ë¡œ ë°˜í™˜ (Streamlit ì—…ë¡œë“œ ê°ì²´ â†’ ì‹¤ì œ íŒŒì¼)
    - PDF, DOCX, PPTX ëª¨ë‘ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    """
    tmp_dir = Path(tempfile.mkdtemp(prefix="st_docs_"))
    out_path = tmp_dir / file.name
    out_path.write_bytes(file.getbuffer())
    logger.info(f"ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ ê²½ë¡œ: {out_path}")
    return out_path


def _load_document(path: Path):
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ë¬¸ì„œ ë¡œë” ë°˜í™˜"""
    ext = path.suffix.lower()
    if ext == ".pdf":
        return PyPDFLoader(str(path))
    if ext == ".docx":
        return Docx2txtLoader(str(path))
    if ext in (".ppt", ".pptx"):
        return UnstructuredPowerPointLoader(str(path))
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")


# =========================
# ìºì‹œ: ì„ë² ë”©/ìŠ¤í”Œë¦¬í„°
# =========================
@st.cache_resource(show_spinner=False)
def get_hf_embeddings():
    """í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹œ)
    - ìµœì´ˆ 1íšŒë§Œ ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ì´í›„ ì„¸ì…˜ì—ì„œëŠ” ì¬ì‚¬ìš©
    - normalize_embeddings=True ëŠ” FAISS ê²€ìƒ‰ ì•ˆì •ì„±ì— ë„ì›€
    - Streamlit Cloudì—ì„œ ëª¨ë¸ ìºì‹œ ë””ë ‰í„°ë¦¬ë¥¼ /tmp/hf ë¡œ ì§€ì •
    """
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
        cache_folder="/tmp/hf",
    )


@st.cache_resource(show_spinner=False)
def get_splitter():
    """í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°(ìºì‹œ)
    - chunk_size/overlap íŠœë‹: ì²­í¬ ìˆ˜ ê°ì†Œ â†’ ì¸ë±ì‹± ì†ë„ í–¥ìƒ
    - ë¬¸ì„œ ì„±ê²©ì— ë”°ë¼ 1800/80 â†’ 2000/60 ë“±ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥
    """
    return RecursiveCharacterTextSplitter(chunk_size=1800, chunk_overlap=80)


# =========================
# ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ
# =========================
def build_vectorstore(doc_paths: List[Path]):
    """ì—…ë¡œë“œ ë¬¸ì„œë“¤ì„ ë¡œë“œâ†’ì²­í¬â†’ì„ë² ë”©â†’FAISS ì¸ë±ìŠ¤ ìƒì„±
    - PDFê°€ ìŠ¤ìº” ì´ë¯¸ì§€ì¼ ê²½ìš° íŒŒì‹±ì´ ëŠë¦´ ìˆ˜ ìˆìŒ(ê°€ëŠ¥í•˜ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF ê¶Œì¥)
    """
    docs = []
    for p in doc_paths:
        loader = _load_document(p)
        docs.extend(loader.load())

    splitter = get_splitter()
    splits = splitter.split_documents(docs)

    embeddings = get_hf_embeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore


# =========================
# ì²´ì¸ êµ¬ì„± (LLM + Retriever + ë©”ëª¨ë¦¬)
# =========================
def get_chain(vectorstore, openai_api_key: str):
    """ConversationalRetrievalChain êµ¬ì„±
    - LLMì€ OpenAI (gpt-4o-mini) ì‚¬ìš©
    - retrieverëŠ” FAISS.as_retriever(search_type="mmr") ì‚¬ìš©
    - ë©”ëª¨ë¦¬ëŠ” ëŒ€í™” ê¸°ë¡ì„ LLMì— ì œê³µ (ë‹µë³€ í’ˆì§ˆ í–¥ìƒ)
    """
    llm = ChatOpenAI(
        openai_api_key=openai_api_key,
        model="gpt-4o-mini",  # ì €ë ´ & ë¹ ë¦„ (í•„ìš” ì‹œ gpt-4o ë¡œ êµì²´ ê°€ëŠ¥)
        temperature=0,
        max_retries=3,  # ê°„ë‹¨í•œ ì¬ì‹œë„ (429 ë“± ë ˆì´íŠ¸ë¦¬ë°‹ ëŒ€ë¹„)
        timeout=30,     # ë„ˆë¬´ ì˜¤ë˜ ê¸°ë‹¤ë¦¬ì§€ ì•Šë„ë¡ íƒ€ì„ì•„ì›ƒ
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
    )

    retriever = vectorstore.as_retriever(search_type="mmr")

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",             # ê°„ë‹¨í•˜ê²Œ ë¬¸ì„œ ìŠ¤í„°í•‘ ë°©ì‹ ì‚¬ìš©(í•„ìš”ì‹œ map_reduce ë“± ë³€ê²½ ê°€ëŠ¥)
        memory=memory,
        get_chat_history=lambda h: h,   # ëŒ€í™” ë‚´ì—­ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
        return_source_documents=True,   # ì†ŒìŠ¤ ë¬¸ì„œ ë°˜í™˜(ê·¼ê±° í‘œì‹œìš©)
        verbose=True,                   # ë””ë²„ê¹… ë¡œê·¸
    )
    return chain


# =========================
# ì‚¬ì´ë“œë°”(UI): API í‚¤/ë¬¸ì„œ ì—…ë¡œë“œ/ì¸ë±ìŠ¤ ë²„íŠ¼
# =========================
with st.sidebar:
    st.subheader("ğŸ”‘ OpenAI API Key")

    # ê¸°ë³¸ê°’: Streamlit Secretsì— OPENAI_API_KEYê°€ ìˆë‹¤ë©´ ìë™ ì‚¬ìš©
    default_key = st.secrets.get("OPENAI_API_KEY", "") if hasattr(st, "secrets") else ""
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        value=default_key,
        help="Streamlit Cloudì˜ Secretsì— OPENAI_API_KEYë¥¼ ë“±ë¡í•´ ë‘ë©´ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.",
    )

    uploaded_files = st.file_uploader(
        "ë¬¸ì„œ ì—…ë¡œë“œ (PDF/DOCX/PPTX)",
        type=["pdf", "docx", "pptx"],
        accept_multiple_files=True,
    )

    build_btn = st.button("ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±")


# =========================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =========================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chain" not in st.session_state:
    st.session_state.chain = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []


# =========================
# ì¸ë±ìŠ¤ ë¹Œë“œ ì‹¤í–‰
# =========================
if build_btn:
    if not openai_api_key:
        st.error("ğŸ”‘ OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not uploaded_files:
        st.warning("ìµœì†Œ 1ê°œ ì´ìƒì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        with st.spinner("ë¬¸ì„œ ì¸ë±ì‹± ì¤‘â€¦ (ìµœì´ˆ 1íšŒëŠ” ëª¨ë¸ ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            try:
                doc_paths = [_persist_upload(f) for f in uploaded_files]
                vs = build_vectorstore(doc_paths)
                st.session_state.vectorstore = vs
                st.session_state.chain = get_chain(vs, openai_api_key)
                st.success("âœ… Vector Index ìƒì„± ì™„ë£Œ!")
            except Exception as e:
                logger.exception("Vector Index ì‹¤íŒ¨")
                st.error(f"ğŸ˜– ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")


# =========================
# ì§ˆì˜ UI
# =========================
st.divider()
st.subheader("ğŸ’¬ ë¬¸ì„œ ê¸°ë°˜ ìì—°ì–´ ì§ˆë¬¸")
user_q = st.text_input("ì§ˆë¬¸ ì…ë ¥:", placeholder="ì˜ˆ: ì—…ë¡œë“œí•œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì§ˆë¬¸í•´  ì£¼ì„¸ìš”")
ask = st.button("ì§ˆë¬¸í•˜ê¸°")


# =========================
# QA ì‹¤í–‰
# =========================
if ask:
    if not openai_api_key:
        st.error("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif st.session_state.chain is None:
        st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    elif not user_q.strip():
        st.info("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘â€¦"):
            try:
                result = st.session_state.chain({"question": user_q})
                answer = result.get("answer", "(ë‹µë³€ ì—†ìŒ)")
                sources = result.get("source_documents", [])

                # í™”ë©´ìš© ê°„ë‹¨í•œ ëŒ€í™” ê¸°ë¡ (ë©”ëª¨ë¦¬ì—ë„ ì €ì¥ë˜ì§€ë§Œ UIì— ë‹¤ì‹œ ë³´ì—¬ì£¼ê¸° ìœ„í•¨)
                st.session_state.chat_history.append(("user", user_q))
                st.session_state.chat_history.append(("assistant", answer))

                st.markdown("### ğŸ§  ë‹µë³€")
                st.write(answer)

                # ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
                if sources:
                    st.markdown("### ğŸ“ ì°¸ê³  ë¬¸ì„œ")
                    with st.expander("ì°¸ê³  ë¬¸ì„œì™€ ì›ë¬¸ ì¼ë¶€ ë³´ê¸°"):
                        for i, doc in enumerate(sources, 1):
                            src = doc.metadata.get("source", f"source_{i}")
                            st.markdown(f"**{i}.** {src}")
                            preview = (doc.page_content or "").strip()
                            if len(preview) > 600:
                                preview = preview[:600] + " â€¦"
                            st.code(preview)
            except Exception as e:
                logger.exception("ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨")
                st.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")


# =========================
# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
# =========================
if st.session_state.chat_history:
    st.divider()
    st.subheader("ğŸ—‚ï¸ í˜„ì¬ ì„¸ì…˜ ëŒ€í™” ê¸°ë¡")
    for role, msg in st.session_state.chat_history[-10:]:
        if role == "user":
            st.markdown(f"**You:** {msg}")
        else:
            st.markdown(f"**Assistant:** {msg}")
