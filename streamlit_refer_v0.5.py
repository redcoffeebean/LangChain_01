"""
streamlit_refer.py â€” RAG(Chat + Retrieval) ì•± (OpenAI LLM + HuggingFace ì„ë² ë”©)

[êµ¬ì„± ìš”ì•½]
- LLM: OpenAI Chat (gpt-4o-mini; ì €ë ´/ë¹ ë¦„)
- Embeddings: HuggingFace (paraphrase-MiniLM-L6-v2)
- ë¬¸ì„œ ë¡œë”: PDF, DOCX, PPTX
- ë²¡í„°ìŠ¤í† ì–´: FAISS
- ì²´ì¸: ConversationalRetrievalChain (ëŒ€í™”í˜• RAG)
- í´ë°±: ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ LLM-onlyë¡œ ê°„ë‹¨ ë‹µë³€ + "RAG: OFF" ì•ˆë‚´

[ì™œ ì´ë ‡ê²Œ?]
- ë¹„ìš©/ìœ ì—°ì„±: LLMì€ OpenAI, ì„ë² ë”©ì€ HFë¡œ ë¶„ë¦¬ â†’ ë¹„ìš©Â·ì„±ëŠ¥ ê· í˜•
- ìµœì‹  LangChain íŒ¨í‚¤ì§€ ë¶„ë¦¬ ë°˜ì˜: langchain-openai / langchain-huggingface / langchain-community
- ì´ˆê¸° ë¡œë“œ/ì¸ë±ì‹± ì„±ëŠ¥: ì„ë² ë”©/ìŠ¤í”Œë¦¬í„° ìºì‹œ(@st.cache_resource)
- ì•ˆì •ì„±: LLM í˜¸ì¶œì— timeout/retries, FAISS ë¯¸ì„¤ì¹˜ ì•ˆë‚´ ì²˜ë¦¬

# =============================
# requirements.txt  
# =============================
streamlit>=1.32                  # ì›¹ UI í”„ë ˆì„ì›Œí¬ â€” Python ì½”ë“œë¡œ ëŒ€í™”í˜• ì›¹ì•± ìƒì„±
langchain>=0.2.0                  # LangChain í•µì‹¬ íŒ¨í‚¤ì§€ â€” ì²´ì¸, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ë“± ê³µí†µ ê¸°ëŠ¥
langchain-community>=0.2.0        # LangChain ì»¤ë®¤ë‹ˆí‹° ëª¨ë“ˆ â€” ë²¡í„°ìŠ¤í† ì–´, ë¡œë” ë“± ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ í†µí•©
langchain-openai>=0.1.0           # OpenAI LLM ì—°ë™ ëª¨ë“ˆ â€” ChatOpenAI, OpenAIEmbeddings ë“± ì œê³µ
langchain-text-splitters>=0.2.0   # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ëª¨ë“ˆ â€” ë¬¸ì„œ ì²­í¬ ë¶„í•  ê¸°ëŠ¥ ì œê³µ (í† í°/ë¬¸ì ë‹¨ìœ„)
langchain-huggingface>=0.1.0      # HuggingFace ì„ë² ë”© ëª¨ë“ˆ â€” sentence-transformers ë“± ë¡œë“œ/ì‚¬ìš© ì§€ì›

sentence-transformers             # HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë”©ìš© â€” ì˜ˆ: paraphrase-MiniLM-L6-v2
torch                              # PyTorch â€” sentence-transformers ëª¨ë¸ ì‹¤í–‰ í•„ìˆ˜ ì˜ì¡´ì„±

pypdf                              # PDF íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
docx2txt                          # DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
unstructured                      # ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ë¡œë” â€” PPTX, HTML ë“± ë¹„ì •í˜• ë°ì´í„° ì²˜ë¦¬
unstructured[pptx]                 # PPT/PPTX ë¬¸ì„œ ì „ìš© ë¡œë” ì§€ì› (python-pptx í¬í•¨)

tiktoken                           # OpenAI ê³µì‹ í† í¬ë‚˜ì´ì € â€” í† í° ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í•  ê°€ëŠ¥
loguru                             # Python ë¡œê¹… ë¼ì´ë¸ŒëŸ¬ë¦¬ â€” ê¹”ë”í•˜ê³  ê°•ë ¥í•œ ë¡œê·¸ ì¶œë ¥

faiss-cpu                          # Facebook AI Similarity Search â€” CPU ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ ì—”ì§„
                                   # Streamlit Cloud/ì¼ë°˜ CPU í™˜ê²½, (GPU í™˜ê²½ì€ faiss-gpuë¡œ êµì²´ ê°€ëŠ¥)
"""
 
import os                          # ìš´ì˜ì²´ì œ ê²½ë¡œ, í™˜ê²½ë³€ìˆ˜ ì œì–´ â€” íŒŒì¼ ì €ì¥, ê²½ë¡œ ì¡°ì‘ ë“±ì— ì‚¬ìš©
import io                          # ë©”ëª¨ë¦¬ ë²„í¼ I/O â€” BytesIO, StringIO ë“± íŒŒì¼ì²˜ëŸ¼ ë‹¤ë£¨ëŠ” ê°ì²´ ì œê³µ
import tempfile                    # ì„ì‹œ íŒŒì¼/í´ë” ìƒì„± â€” ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ í›„ ì²˜ë¦¬ì— ì‚¬ìš©
from pathlib import Path           # ê²½ë¡œ ê°ì²´í™” â€” ê²½ë¡œ ì¡°ì‘ì„ ì§ê´€ì ì´ê³  í”Œë«í¼ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰
from typing import List, Optional  # íƒ€ì… íŒíŠ¸ â€” List, Optional ë“±ìœ¼ë¡œ í•¨ìˆ˜ ì¸ì/ë°˜í™˜ íƒ€ì… ëª…ì‹œ
import streamlit as st             # Streamlit â€” ëŒ€í™”í˜• ì›¹ ì•± UI ìƒì„± ë¼ì´ë¸ŒëŸ¬ë¦¬
from loguru import logger          # Loguru â€” ê¹”ë”í•˜ê³  ê°•ë ¥í•œ ë¡œê¹… ê¸°ëŠ¥ ì œê³µ

# =========================
# LangChain ê´€ë ¨ ì„í¬íŠ¸
# =========================
# (ì¤‘ìš”) ChatOpenAIëŠ” langchain_openaiì—ì„œ ì œê³µ
from langchain_openai import ChatOpenAI

# (ì¤‘ìš”) í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”©ì€ langchain_huggingfaceì—ì„œ ì œê³µ
#  - ì˜ˆì „ì˜ langchain_community.embeddings.HuggingFaceEmbeddingsëŠ” 0.2.2ì—ì„œ Deprecated
from langchain_huggingface import HuggingFaceEmbeddings

# ì»¤ë®¤ë‹ˆí‹° ë²¡í„°ìŠ¤í† ì–´/ë¡œë” (FAISS ì„í¬íŠ¸ëŠ” í™˜ê²½ì— ë”°ë¼ ì‹¤íŒ¨ ê°€ëŠ¥ â†’ ì•ˆì „ ê°€ë“œ)
try:
    from langchain_community.vectorstores import FAISS
except Exception as _e:  # ImportError í¬í•¨
    FAISS = None
    _faiss_import_err = _e

from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredPowerPointLoader,
    TextLoader,  # â† TXT íŒŒì¼ ë¡œë” ì¶”ê°€
)

# í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°(ë³„ë„ íŒ¨í‚¤ì§€)
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ì²´ì¸/ë©”ëª¨ë¦¬
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# (LLM-only í´ë°±ìš©) ë©”ì‹œì§€ íƒ€ì…
from langchain.schema import SystemMessage, HumanMessage

# í† í°ë¼ì´ì €(tiktoken): ëª¨ë¸ í† í° ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ìë¥´ê¸° ìœ„í•¨
try:
    import tiktoken
except Exception as _tk_err:
    tiktoken = None
    _tiktoken_import_err = _tk_err


# =========================
# í† í° ì²­í¬ ì„¤ì • (í•„ìš” ì‹œ ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨)
# =========================
TOKEN_ENCODING_NAME = "cl100k_base"  # GPT-4/4o ê³„ì—´ê³¼ í˜¸í™˜ë˜ëŠ” ì¼ë°˜ì  ì¸ì½”ë”©
TOKEN_CHUNK_SIZE = 800               # ì²­í¬ í•˜ë‚˜ì˜ ìµœëŒ€ í† í° ìˆ˜ (ë¬¸ì ì•„ë‹˜!)
TOKEN_CHUNK_OVERLAP = 80             # ì²­í¬ ê°„ ê²¹ì¹¨(í† í° ë‹¨ìœ„)


# =========================
# Streamlit ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="RAG Chatbot", page_icon="ğŸ¤–")
st.title("RAG Chatbot âœ¨")
st.caption("ë¬¸ì„œê°€ ì—…ë¡œë“œëœ ê²½ìš°ëŠ” RAGë¡œ ë‹µë³€í•˜ê³ , ì—…ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°ëŠ” LLMì—ì„œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")


# =========================
# ìœ í‹¸/í—¬í¼
# =========================
def _persist_upload(file) -> Path:
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ í´ë”ì— ì €ì¥ í›„ ê²½ë¡œ ë°˜í™˜ (Streamlit ì—…ë¡œë“œ ê°ì²´ â†’ ì‹¤ì œ íŒŒì¼)
    - PDF, DOCX, PPTX ëª¨ë‘ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    """
    tmp_dir = Path(tempfile.mkdtemp(prefix="st_docs_"))
    out_path = tmp_dir / file.name
    out_path.write_bytes(file.getbuffer())
    logger.info(f"ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ ê²½ë¡œ: {out_path}")
    return out_path


def _load_document(path: Path):
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ë¬¸ì„œ ë¡œë” ë°˜í™˜"""
    ext = path.suffix.lower()
    if ext == ".pdf":
        return PyPDFLoader(str(path))
    if ext == ".docx":
        return Docx2txtLoader(str(path))
    if ext in (".ppt", ".pptx"):
        return UnstructuredPowerPointLoader(str(path))
    if ext == ".txt":
        # TXT íŒŒì¼ ë¡œë” (UTF-8 ê¸°ì¤€)
        return TextLoader(str(path), encoding="utf-8")
    raise ValueError(f"ğŸ˜– ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")


# =========================
# ìºì‹œ: ì„ë² ë”©/ìŠ¤í”Œë¦¬í„°
# =========================
@st.cache_resource(show_spinner=False)
def get_hf_embeddings():
    """í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹œ)
    - ìµœì´ˆ 1íšŒë§Œ ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ì´í›„ ì„¸ì…˜ì—ì„œëŠ” ì¬ì‚¬ìš©
    - normalize_embeddings=True ëŠ” FAISS ê²€ìƒ‰ ì•ˆì •ì„±ì— ë„ì›€
    - Streamlit Cloudì—ì„œ ëª¨ë¸ ìºì‹œ ë””ë ‰í„°ë¦¬ë¥¼ /tmp/hf ë¡œ ì§€ì •
    """
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
        cache_folder="/tmp/hf",
    )


def _get_tiktoken_encoding(name: str):
    """tiktoken ì¸ì½”ë”© í•¸ë“¤ëŸ¬ íšë“
    - tiktokenì´ ì—†ìœ¼ë©´ ì¹œì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€
    - ì¡´ì¬í•˜ë©´ ì§€ì •í•œ ì¸ì½”ë”©ì„ ë°˜í™˜
    """
    if tiktoken is None:
        raise RuntimeError(
            f"tiktokenì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì›ì¸: {repr(_tiktoken_import_err)})\n"
            "requirements.txtì— 'tiktoken'ì„ ì¶”ê°€í•˜ê³  ì¬ë°°í¬í•˜ì„¸ìš”."
        )
    try:
        return tiktoken.get_encoding(name)
    except Exception:
        # cl100k_baseê°€ ëŒ€ë¶€ë¶„ì˜ ìµœì‹  ëª¨ë¸ê³¼ í˜¸í™˜ë˜ë¯€ë¡œ ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
        return tiktoken.get_encoding("cl100k_base")


@st.cache_resource(show_spinner=False)
def get_token_splitter(
    chunk_tokens: int = TOKEN_CHUNK_SIZE,
    overlap_tokens: int = TOKEN_CHUNK_OVERLAP,
    encoding_name: str = TOKEN_ENCODING_NAME,
):
    """í† í° ë‹¨ìœ„ í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°(ìºì‹œ)
    - tiktoken ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•´ 'í† í° ìˆ˜' ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ë¥¼ ìë¦„
    - chunk_tokens / overlap_tokens ì¡°ì ˆë¡œ ê¸¸ì´/ë¹„ìš©/í’ˆì§ˆ ë°¸ëŸ°ìŠ¤ ê°€ëŠ¥
    """
    _ = _get_tiktoken_encoding(encoding_name)  # ì¡´ì¬ í™•ì¸ (ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ)
    # from_tiktoken_encoderëŠ” ë‚´ë¶€ì ìœ¼ë¡œ encoding_name ë¬¸ìì—´ë§Œ í•„ìš”
    return RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=chunk_tokens,
        chunk_overlap=overlap_tokens,
        encoding_name=encoding_name,
    )


# =========================
# ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ
# =========================
def build_vectorstore(doc_paths: List[Path]):
    """ì—…ë¡œë“œ ë¬¸ì„œë“¤ì„ ë¡œë“œâ†’(í† í° ê¸°ì¤€)ì²­í¬â†’ì„ë² ë”©â†’FAISS ì¸ë±ìŠ¤ ìƒì„±
    - PDFê°€ ìŠ¤ìº” ì´ë¯¸ì§€ì¼ ê²½ìš° íŒŒì‹±ì´ ëŠë¦´ ìˆ˜ ìˆìŒ(ê°€ëŠ¥í•˜ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF ê¶Œì¥)
    - FAISS/tiktoken ë¯¸ì„¤ì¹˜ ì‹œ ì¹œì ˆí•œ ì—ëŸ¬ ì•ˆë‚´
    """
    if FAISS is None:
        raise RuntimeError(
            f"ğŸ˜– FAISS ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì›ì¸: {repr(_faiss_import_err)})\n"
            "CPU í™˜ê²½ì—ì„œëŠ” requirements.txtì— 'faiss-cpu'ë¥¼ ì¶”ê°€í•´ ì£¼ì„¸ìš”.\n"
            "GPU(CUDA) í™˜ê²½ì—ì„œëŠ” 'faiss-gpu'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )

    docs = []
    for p in doc_paths:
        loader = _load_document(p)
        docs.extend(loader.load())

    splitter = get_token_splitter()   # â˜… í† í° ë‹¨ìœ„ ìŠ¤í”Œë¦¬í„° ì‚¬ìš©
    splits = splitter.split_documents(docs)

    embeddings = get_hf_embeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore


# =========================
# ì²´ì¸ êµ¬ì„± (LLM + Retriever + ë©”ëª¨ë¦¬)
# =========================
def get_chain(vectorstore, openai_api_key: str):
    """ConversationalRetrievalChain êµ¬ì„±
    - LLMì€ OpenAI (gpt-4o-mini) ì‚¬ìš©
    - retrieverëŠ” FAISS.as_retriever(search_type="mmr") ì‚¬ìš©
    - ë©”ëª¨ë¦¬ëŠ” ëŒ€í™” ê¸°ë¡ì„ LLMì— ì œê³µ (ë‹µë³€ í’ˆì§ˆ í–¥ìƒ)
    """
    llm = ChatOpenAI(
        openai_api_key=openai_api_key,
        model="gpt-4o-mini",  # ì €ë ´ & ë¹ ë¦„ (í•„ìš” ì‹œ gpt-4o ë¡œ êµì²´ ê°€ëŠ¥)
        temperature=0,        # 0: ì°½ì˜ì„± ë‚®ì¶”ê³ , ì¼ê´€ì„± ë†’ì€ ë‹µë³€ ìƒì„±
        max_retries=3,        # ê°„ë‹¨í•œ ì¬ì‹œë„ (429 ë“± ë ˆì´íŠ¸ë¦¬ë°‹ ëŒ€ë¹„)
        timeout=10,           # 10ì´ˆ ì´ìƒ ê±¸ë¦¬ë©´ ìš”ì²­ ì¤‘ë‹¨
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",  # ì²´ì¸ ë‚´ë¶€ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ì°¾ì„ ë•Œ ì“°ëŠ” í‚¤ ì´ë¦„
        return_messages=True,       # ëŒ€í™” ë©”ì‹œì§€ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        output_key="answer",        # ìµœì¢… ìƒì„±ëœ ë‹µë³€ì˜ í‚¤ ì´ë¦„
    )

    retriever = vectorstore.as_retriever(search_type="mmr")
    #  - vectorstore: ì—…ë¡œë“œëœ ë¬¸ì„œ ì„ë² ë”©ì„ ì €ì¥í•œ FAISS ë²¡í„° DB
    #  - retriever: ì§ˆë¬¸ì„ ì„ë² ë”©í•˜ì—¬ ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰
    #  - search_type="mmr": ì¤‘ë³µì„ ì¤„ì´ê³  ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ëŠ” ê²€ìƒ‰ ì „ëµ

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",           # ì²­í¬ë¥¼ ê·¸ëŒ€ë¡œ í”„ë¡¬í”„íŠ¸ì— ìŠ¤í„°í•‘(stuff)í•˜ëŠ” ê°„ë‹¨í•œ ë°©ì‹
        memory=memory,
        get_chat_history=lambda h: h, # ëŒ€í™” ë‚´ì—­ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
        return_source_documents=True, # ì°¸ì¡°í•œ ì›ë¬¸ ì²­í¬ê¹Œì§€ í•¨ê»˜ ë°˜í™˜
        verbose=True,                 # ë””ë²„ê¹… ë¡œê·¸
    )
    return chain


# =========================
# LLM ë‹¨ë…(ë¹„ RAG) ì‘ë‹µ í—¬í¼
# =========================
def answer_without_rag(question: str, openai_api_key: str) -> str:
    """ë¬¸ì„œ ì¸ë±ìŠ¤ê°€ ì—†ì„ ë•Œ, LLMë§Œìœ¼ë¡œ ê°„ê²°í•œ ë‹µë³€ì„ ìƒì„±
    - 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ì§§ê³  í•µì‹¬ë§Œ
    - RAGê°€ ì•„ë‹˜ì„ UIì—ì„œ ë³„ë„ ì•ˆë‚´
    """
    llm = ChatOpenAI(
        openai_api_key=openai_api_key,
        model="gpt-4o-mini",
        temperature=0,
        max_retries=3,
        timeout=10,
    )
    sys = SystemMessage(content="ë„ˆëŠ” ê°„ê²°í•œ ì¡°ìˆ˜ë‹¤. ëª¨ë“  ë‹µë³€ì€ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ë§í•´ë¼.")
    user = HumanMessage(content=question)
    resp = llm.invoke([sys, user])  # Chat ëª¨ë¸ì˜ invoke ì‚¬ìš©
    return getattr(resp, "content", str(resp))


# =========================
# ì‚¬ì´ë“œë°”(UI): API í‚¤/ë¬¸ì„œ ì—…ë¡œë“œ/ì¸ë±ìŠ¤ ë²„íŠ¼
# =========================
with st.sidebar:
    st.subheader("ğŸ”‘ OpenAI API Key")

    # ê¸°ë³¸ê°’: Streamlit Secretsì— OPENAI_API_KEYê°€ ìˆë‹¤ë©´ ìë™ ì‚¬ìš©
    default_key = st.secrets.get("OPENAI_API_KEY", "") if hasattr(st, "secrets") else ""
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        value=default_key,
        help="Streamlit Cloudì˜ Secretsì— OPENAI_API_KEYë¥¼ ë“±ë¡í•˜ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.",
    )

    uploaded_files = st.file_uploader(
        "ë¬¸ì„œ ì—…ë¡œë“œ (PDF/DOCX/PPTX)",
        type=["pdf", "docx", "pptx"],
        accept_multiple_files=True,
    )

    build_btn = st.button("ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±")


# =========================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =========================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chain" not in st.session_state:
    st.session_state.chain = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []


# =========================
# ì¸ë±ìŠ¤ ë¹Œë“œ ì‹¤í–‰
# =========================
if build_btn:
    if not openai_api_key:
        st.error("ğŸ”‘ OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not uploaded_files:
        st.warning("ìµœì†Œ 1ê°œ ì´ìƒì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        with st.spinner("ë²¡í„° ì¸ë±ì‹± ì¤‘â€¦ (ìµœì´ˆì—ëŠ” ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            try:
                doc_paths = [_persist_upload(f) for f in uploaded_files]
                vs = build_vectorstore(doc_paths)
                st.session_state.vectorstore = vs
                st.session_state.chain = get_chain(vs, openai_api_key)
                st.success("âœ… Vector Index ìƒì„± ì™„ë£Œ! (RAG ê°€ëŠ¥)")
            except Exception as e:
                logger.exception("Vector Index ì‹¤íŒ¨")
                st.error(f"ğŸ˜– ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")


# =========================
# ì§ˆì˜ UI
# =========================
st.divider()
st.subheader("ğŸ’¬ ë¬¸ì„œ ê¸°ë°˜ ìì—°ì–´ ì§ˆë¬¸")
user_q = st.text_input("ì§ˆë¬¸ ì…ë ¥:", placeholder="ì˜ˆ: ì—…ë¡œë“œí•œ ë¬¸ì„œì˜ í•µì‹¬ë§Œ ê°„ë‹¨íˆ ì•Œë ¤ì£¼ì„¸ìš”")
ask = st.button("ì§ˆë¬¸í•˜ê¸°")


# =========================
# QA ì‹¤í–‰ (RAG ON/OFF í´ë°± í¬í•¨)
# =========================
if ask:
    if not openai_api_key:
        st.error("ğŸ”‘ OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not user_q.strip():
        st.info("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        # 1) ì¸ë±ìŠ¤/ì²´ì¸ ì¤€ë¹„ ì—¬ë¶€ í™•ì¸
        if st.session_state.chain is None:
            # ğŸ” í´ë°±: ë¬¸ì„œ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë¯€ë¡œ LLM ë‹¨ë… ê°„ë‹¨ ë‹µë³€
            with st.spinner("LLM ë‹µë³€ ìƒì„± ì¤‘â€¦ (RAG OFF)"):
                try:
                    answer = answer_without_rag(user_q, openai_api_key)
                    st.session_state.chat_history.append(("user", user_q))
                    st.session_state.chat_history.append(("assistant", answer))

                    st.markdown("### ğŸ§  ë‹µë³€  `RAG: OFF`")
                    # st.write(answer)
                    st.text(answer)  #  í•œê¸€/ì˜ë¬¸ ì„œì‹ì„ ì œê±°í•˜ê³  'ìˆœìˆ˜ í…ìŠ¤íŠ¸'ë¡œ í‘œì‹œí•˜ì—¬ ê¸€ê¼´ ì°¨ì´ë¥¼ ì—†ì•°
                    st.info("RAG ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤. ì—…ë¡œë“œí•œ ë¬¸ì„œ/ì¸ë±ìŠ¤ê°€ ì—†ì–´ ì¼ë°˜ LLMìœ¼ë¡œ ê°„ë‹¨ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.exception("LLM-only ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨")
                    st.error(f"ğŸ˜– ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨(LLM-only): {e}")
        else:
            # âœ… RAG ê²½ë¡œ
            with st.spinner("RAG ì‘ë‹µ ìƒì„± ì¤‘â€¦ (RAG ON)"):
                try:
                    result = st.session_state.chain({"question": user_q})
                    answer = result.get("answer", "(ë‹µë³€ ì—†ìŒ)")
                    sources = result.get("source_documents", [])

                    st.session_state.chat_history.append(("user", user_q))
                    st.session_state.chat_history.append(("assistant", answer))

                    st.markdown("### ğŸ§  ë‹µë³€  `RAG: ON`")
                    # st.write(answer)
                    st.text(answer)  #  í•œê¸€/ì˜ë¬¸ ì„œì‹ì„ ì œê±°í•˜ê³  'ìˆœìˆ˜ í…ìŠ¤íŠ¸'ë¡œ í‘œì‹œí•˜ì—¬ ê¸€ê¼´ ì°¨ì´ë¥¼ ì—†ì•°
                  
                    # ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
                    if sources:
                        st.markdown("### ğŸ’¡ ì°¸ê³  ë¬¸ì„œ")
                        with st.expander("ì°¸ê³  ë¬¸ì„œ ìœ„ì¹˜ ë° ì›ë¬¸ ì¼ë¶€ ë³´ê¸°"):
                            for i, doc in enumerate(sources, 1):
                                src = doc.metadata.get("source", f"source_{i}")
                                st.markdown(f"**{i}.** {src}")
                                preview = (doc.page_content or "").strip()
                                if len(preview) > 600:
                                    preview = preview[:600] + " â€¦"
                                st.code(preview)
                    else:
                        st.info("í•´ë‹¹ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ì¸ë±ì‹± ë²”ìœ„ë¥¼ ëŠ˜ë ¤ ë³´ì„¸ìš”.)")
                except Exception as e:
                    logger.exception("ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨(RAG)")
                    st.error(f"ğŸ˜– ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨(RAG): {e}")


# =========================
# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
# =========================
if st.session_state.chat_history:
    st.divider()
    st.subheader("ğŸ—‚ï¸ ì„¸ì…˜ ì•„ì¹´ì´ë¸Œ")
    for role, msg in st.session_state.chat_history[-10:]:
        if role == "user":
            st.markdown(f"**You:** {msg}")
        else:
            st.markdown(f"**Assistant:** {msg}")

