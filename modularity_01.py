# ==================================================
# RAG Single-File Template (ëª¨ë“ˆì‹ êµì²´ê°€ ì‰¬ìš´ ë‹¨ì¼ íŒŒì¼ êµ¬ì¡°)
# ==================================================
# ëª©ì : í•œ íŒŒì¼ ì•ˆì—ì„œ Loader / Splitter / Embeddings / VectorStore / LLM / Chain / UI
#       ê° ê¸°ëŠ¥ì„ êµ¬íší•˜ê³ , ì„¤ì •(config) ë˜ëŠ” ì„ íƒ(selectbox)ë§Œ ë°”ê¾¸ë©´ êµ¬í˜„ì²´ë¥¼ êµì²´í•  ìˆ˜ ìˆê²Œ í•¨.
# ì‚¬ìš©: Streamlitë¡œ ì‹¤í–‰ (ì˜ˆ: `streamlit run streamlit_rag_singlefile_template.py`)
# ì£¼ì˜: í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
#   pip install streamlit langchain-community langchain-text-splitters langchain-openai langchain-huggingface faiss-cpu chromadb tiktoken langchain-pinecone pinecone
#   (ì„ íƒ) transformers accelerate sentence-transformers
# ==================================================


# ##################################################
# ëª©ì°¨ (Flow Overview)
# ##################################################
# 1) CONFIG & REGISTRY                         â€” êµ¬í˜„ ì„ íƒ, ê¸°ë³¸ ì˜µì…˜
# 2) ê³µí†µ íƒ€ì… & ìœ í‹¸                          â€” ê°„ë‹¨ íƒ€ì…/í—¬í¼
# 3) Loader êµ¬í˜„                               â€” PDF/DOCX/PPT/TXT
# 4) Splitter êµ¬í˜„                             â€” tiktoken/char ê¸°ë°˜
# 5) Embeddings êµ¬í˜„                           â€” HuggingFace/OpenAI
# 6) VectorStore êµ¬í˜„                          â€” FAISS/Chroma/Pinecone/(Oracle placeholder)
# 7) LLM êµ¬í˜„                                  â€” OpenAI(ê¸°ë³¸), (ì˜µì…˜) ë¡œì»¬
# 8) Chain Builder                             â€” ConversationalRetrievalChain ì¡°ë¦½
# 9) Streamlit UI                              â€” íŒŒì¼ ì—…ë¡œë“œ, êµ¬ì„± ì„ íƒ, ì±„íŒ…


# ##################################################
# 1) CONFIG & REGISTRY
# ##################################################
from __future__ import annotations
import os
from dataclasses import dataclass
from typing import List, Dict, Any, Callable
import importlib.util
import time, math

DEFAULT_CONFIG = {
    "embeddings": "hf:minilm",   # 'hf:minilm' | 'hf:paraphrase' | 'openai'
    "vectorstore": "faiss",      # 'faiss' | 'chroma' | 'oracle' (placeholder)
    "splitter": "tiktoken",      # 'tiktoken' | 'char'
    "llm": "openai:gpt-4o-mini", # 'openai:gpt-4o-mini' | (ì˜µì…˜) 'local:...'
    "chunk_size": 800,
    "chunk_overlap": 100,
}

# ì„ íƒì§€ ë¼ë²¨ â†’ ë‚´ë¶€ í‚¤ ë§¤í•‘ (UIì— í‘œì‹œë  ë³´ê¸°)
UI_CHOICES = {
    "Embeddings": [
        ("HuggingFace â€” all-MiniLM-L6-v2", "hf:minilm"),
        ("HuggingFace â€” paraphrase-MiniLM-L6-v2", "hf:paraphrase"),
        ("OpenAI â€” text-embedding-3-large", "openai"),
    ],
    "VectorStore": [
        ("FAISS (in-memory)", "faiss"),
        ("ChromaDB (local client)", "chroma"),
        ("Pinecone (managed)", "pinecone"),
        ("Oracle Vector (placeholder)", "oracle"),
    ],
    "Splitter": [
        ("tiktoken ê¸°ë°˜ TokenSplitter", "tiktoken"),
        ("ë¬¸ì ê¸°ë°˜ RecursiveCharacter", "char"),
    ],
    "LLM": [
        ("OpenAI â€” gpt-4o-mini", "openai:gpt-4o-mini"),
    ],
}


# ##################################################
# 2) ê³µí†µ íƒ€ì… & ìœ í‹¸
# ##################################################
@dataclass
class LoadedDoc:
    page_content: str
    metadata: Dict[str, Any]


def _ensure(msg: str, ok: bool):
    if not ok:
        raise RuntimeError(msg)

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—¬ë¶€ ì²´í¬ (ì˜ˆ: chromadb ìœ ë¬´ì— ë”°ë¼ UIì—ì„œ ìë™ ëŒ€ì²´)
def is_pkg_available(module_name: str) -> bool:
    try:
        return importlib.util.find_spec(module_name) is not None
    except Exception:
        return False

# sqlite3 ìµœì†Œ ë²„ì „(Chroma ìš”êµ¬: >= 3.35.0) ì¶©ì¡± ì—¬ë¶€ ì ê²€
def is_sqlite_supported(min_major=3, min_minor=35, min_patch=0) -> bool:
    try:
        import sqlite3
        v = getattr(sqlite3, "sqlite_version_info", None)
        if not v:
            ver = getattr(sqlite3, "sqlite_version", "0.0.0")
            parts = [int(x) for x in ver.split(".")]
            while len(parts) < 3:
                parts.append(0)
            v = tuple(parts[:3])
        return tuple(v) >= (min_major, min_minor, min_patch)
    except Exception:
        return False

# --- Helper: í˜„ì¬ VectorStoreê°€ ì‹¤ì§ˆì ìœ¼ë¡œ FAISSì¸ì§€ íŒë‹¨ (Chroma ë‚´ë¶€ í´ë°± í¬í•¨) ---
def is_faiss_backed(provider) -> bool:
    try:
        from langchain_community.vectorstores import FAISS as FAISSClass
    except Exception:
        FAISSClass = None
    if isinstance(provider, FaissVS):
        return True
    vsv = getattr(provider, "vs", None)
    if vsv is None:
        return False
    if FAISSClass is not None and isinstance(vsv, FAISSClass):
        return True
    # Heuristic: FAISS ì¸ë±ìŠ¤ëŠ” .index.ntotal íŠ¹ì„±ì´ ì¡´ì¬
    idx = getattr(vsv, "index", None)
    return hasattr(idx, "ntotal")

# --- Helper: ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ VectorStore ì´ë¦„ ---
def effective_vs_name(provider) -> str:
    if is_faiss_backed(provider):
        return "faiss"
    # Chroma/Pinecone ê°ì§€ (ê°€ëŠ¥í•˜ë©´)
    vsv = getattr(provider, "vs", None)
    try:
        from langchain_community.vectorstores import Chroma
        if isinstance(vsv, Chroma):
            return "chroma"
    except Exception:
        pass
    try:
        from langchain_pinecone import PineconeVectorStore
        if isinstance(vsv, PineconeVectorStore):
            return "pinecone"
    except Exception:
        pass
    return type(provider).__name__


# ##################################################
# 3) Loader êµ¬í˜„ â€” PDF/DOCX/PPT/TXT (langchain-community ê¶Œì¥)
# ##################################################
from pathlib import Path

def load_documents(paths: List[str]) -> List[Any]:
    """íŒŒì¼ í™•ì¥ìë³„ Loader ì„ íƒ í›„ Document ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    ë°˜í™˜: langchainì˜ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ (downstream í˜¸í™˜)
    """
    docs: List[Any] = []
    for p in paths:
        ext = Path(p).suffix.lower()
        if ext == ".pdf":
            try:
                from langchain_community.document_loaders import PyPDFLoader
            except Exception as e:
                raise RuntimeError("PyPDFLoader ì‚¬ìš©ì„ ìœ„í•´ 'langchain-community'ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
            loader = PyPDFLoader(p)
            docs.extend(loader.load())
        elif ext in (".docx", ".doc"):
            try:
                from langchain_community.document_loaders import Docx2txtLoader
            except Exception as e:
                raise RuntimeError("Docx2txtLoader ì‚¬ìš©ì„ ìœ„í•´ 'langchain-community'ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
            loader = Docx2txtLoader(p)
            docs.extend(loader.load())
        elif ext in (".pptx", ".ppt"):
            try:
                from langchain_community.document_loaders import UnstructuredPowerPointLoader
            except Exception as e:
                raise RuntimeError("UnstructuredPowerPointLoader ì‚¬ìš©ì„ ìœ„í•´ 'unstructured' ê³„ì—´ ì˜ì¡´ì„±ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.") from e
            loader = UnstructuredPowerPointLoader(p)
            docs.extend(loader.load())
        else:  # ê¸°ë³¸ TXT/ê¸°íƒ€ëŠ” TextLoader ì‹œë„
            try:
                from langchain_community.document_loaders import TextLoader
            except Exception as e:
                raise RuntimeError("TextLoader ì‚¬ìš©ì„ ìœ„í•´ 'langchain-community'ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
            loader = TextLoader(p, encoding="utf-8")
            docs.extend(loader.load())
    return docs


# ##################################################
# 4) Splitter êµ¬í˜„ â€” tiktoken / char
# ##################################################

def get_splitter(kind: str, chunk_size: int, chunk_overlap: int):
    if kind == "tiktoken":
        try:
            from langchain_text_splitters import TokenTextSplitter
        except Exception as e:
            raise RuntimeError("tiktoken ê¸°ë°˜ splitterë¥¼ ìœ„í•´ 'langchain-text-splitters'ì™€ 'tiktoken'ì´ í•„ìš”í•©ë‹ˆë‹¤.") from e
        return TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    else:
        # ë¬¸ì ê¸°ë°˜ ë¶„í•  (RecursiveCharacterTextSplitter)
        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
        except Exception as e:
            raise RuntimeError("ë¬¸ì ê¸°ë°˜ splitterë¥¼ ìœ„í•´ 'langchain-text-splitters' ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
        return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)


def split_documents(docs: List[Any], splitter) -> List[Any]:
    return splitter.split_documents(docs)


# ##################################################
# 5) Embeddings êµ¬í˜„ â€” HuggingFace / OpenAI
# ##################################################
class EmbeddingsProvider:
    def embed_documents(self, texts: List[str]) -> List[List[float]]: ...
    def embed_query(self, text: str) -> List[float]: ...


class HFEmbeddings(EmbeddingsProvider):
    def __init__(self, model_name: str):
        try:
            from langchain_huggingface import HuggingFaceEmbeddings
        except Exception as e:
            raise RuntimeError("HuggingFace ì„ë² ë”©ì„ ìœ„í•´ 'langchain-huggingface'ì™€ 'sentence-transformers'ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
        self._impl = HuggingFaceEmbeddings(model_name=model_name)

    def embed_documents(self, texts):
        return self._impl.embed_documents(texts)

    def embed_query(self, text):
        return self._impl.embed_query(text)


class OpenAIEmbeddingsProvider(EmbeddingsProvider):
    def __init__(self, model: str = "text-embedding-3-large"):
        try:
            from langchain_openai import OpenAIEmbeddings
        except Exception as e:
            raise RuntimeError("OpenAI ì„ë² ë”©ì„ ìœ„í•´ 'langchain-openai'ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
        _ensure("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”", bool(os.getenv("OPENAI_API_KEY")))
        self._impl = OpenAIEmbeddings(model=model)

    def embed_documents(self, texts):
        return self._impl.embed_documents(texts)

    def embed_query(self, text):
        return self._impl.embed_query(text)


def get_embeddings(name: str) -> EmbeddingsProvider:
    if name.startswith("hf:"):
        if name == "hf:minilm":
            return HFEmbeddings("sentence-transformers/all-MiniLM-L6-v2")
        elif name == "hf:paraphrase":
            return HFEmbeddings("sentence-transformers/paraphrase-MiniLM-L6-v2")
    elif name == "openai":
        return OpenAIEmbeddingsProvider()
    raise ValueError(f"Unknown embeddings provider: {name}")


# ##################################################
# 6) VectorStore êµ¬í˜„ â€” FAISS / Chroma / Pinecone / Oracle(placeholder)
# ##################################################
class VectorStoreProvider:
    def from_documents(self, docs: List[Any], embeddings: EmbeddingsProvider) -> "VectorStoreProvider": ...
    def as_retriever(self, **kwargs): ...
    def persist(self): ...


class FaissVS(VectorStoreProvider):
    def __init__(self):
        self.vs = None
    def from_documents(self, docs, embeddings):
        try:
            from langchain_community.vectorstores import FAISS
        except Exception as e:
            raise RuntimeError("FAISS ì‚¬ìš©ì„ ìœ„í•´ 'faiss-cpu'ì™€ 'langchain-community'ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
        # â˜… ìˆ˜ì •: LangChain ë²„ì „ ì°¨ì´ë¥¼ í¡ìˆ˜í•˜ê¸° ìœ„í•´ ë‚´ë¶€ ì„ë² ë”© êµ¬í˜„ì²´ë¥¼ ì „ë‹¬
        embed_impl = getattr(embeddings, "_impl", embeddings)
        self.vs = FAISS.from_documents(docs, embed_impl)
        return self
    def as_retriever(self, **kwargs):
        return self.vs.as_retriever(**kwargs)
    def persist(self):
        pass  # in-memory ê¸°ë³¸


class ChromaVS(VectorStoreProvider):
    def __init__(self, collection_name: str = "rag_collection"):
        self.collection_name = collection_name
        self.vs = None
    def from_documents(self, docs, embeddings):
        try:
            from langchain_community.vectorstores import Chroma
        except Exception as e:
            raise RuntimeError("Chroma ì‚¬ìš©ì„ ìœ„í•´ 'chromadb'ì™€ 'langchain-community'ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e

        # sqlite ë²„ì „ì´ ë‚®ìœ¼ë©´ FAISSë¡œ ìë™ í´ë°±
        if not is_sqlite_supported():
            try:
                import streamlit as st
                st.warning("ChromaDBëŠ” sqlite3 >= 3.35.0ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ í™˜ê²½ì—ì„œëŠ” **FAISS**ë¡œ ìë™ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            except Exception:
                pass
            from langchain_community.vectorstores import FAISS
            embed_impl = getattr(embeddings, "_impl", embeddings)
            self.vs = FAISS.from_documents(docs, embed_impl)
            return self

        # ë‚´ë¶€ ì„ë² ë”© êµ¬í˜„ì²´ë¥¼ ì „ë‹¬í•˜ì—¬ í˜¸í™˜ì„± ë³´ì¥
        embed_impl = getattr(embeddings, "_impl", embeddings)
        try:
            self.vs = Chroma.from_documents(docs, embed_impl, collection_name=self.collection_name)
            return self
        except Exception as e:
            # ëŸ°íƒ€ì„ì—ì„œ sqlite ê´€ë ¨ ì—ëŸ¬ê°€ ë‚œ ê²½ìš°ì—ë„ FAISSë¡œ í´ë°±
            if "sqlite" in str(e).lower():
                try:
                    import streamlit as st
                    st.warning("ChromaDB(sqlite) ì´ˆê¸°í™” ì‹¤íŒ¨ â†’ **FAISS**ë¡œ ìë™ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                except Exception:
                    pass
                from langchain_community.vectorstores import FAISS
                self.vs = FAISS.from_documents(docs, embed_impl)
                return self
            raise
    def as_retriever(self, **kwargs):
        return self.vs.as_retriever(**kwargs)
    def persist(self):
        try:
            self.vs.persist()
        except Exception:
            pass


# ---- Pinecone VectorStore ----
class PineconeVS(VectorStoreProvider):
    def __init__(self, index_name: str):
        self.index_name = index_name
        self.vs = None

    def _ensure_pkgs(self):
        if not (is_pkg_available("langchain_pinecone") and is_pkg_available("pinecone")):
            raise RuntimeError("Pinecone ì‚¬ìš©ì„ ìœ„í•´ 'langchain-pinecone'ê³¼ 'pinecone' íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    def from_documents(self, docs, embeddings):
        self._ensure_pkgs()
        from langchain_pinecone import PineconeVectorStore
        try:
            from pinecone import Pinecone as PineconeClient, ServerlessSpec
        except Exception:
            from pinecone import Pinecone as PineconeClient  # ServerlessSpec ë¯¸ì‚¬ìš© êµ¬ë²„ì „ í˜¸í™˜
            ServerlessSpec = None

        api_key = os.getenv("PINECONE_API_KEY")
        _ensure("PINECONE_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš” (ì‚¬ì´ë“œë°”ì— ì…ë ¥)", bool(api_key))

        pc = PineconeClient(api_key=api_key)

        # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ â†’ ì—†ìœ¼ë©´ ìƒì„±
        need_create = False
        try:
            pc.describe_index(self.index_name)
        except Exception:
            need_create = True

        if need_create:
            # ì„ë² ë”© ì°¨ì› ìë™ ì¶”ë¡ 
            embed_impl = getattr(embeddings, "_impl", embeddings)
            dim = len(embed_impl.embed_query("dimension_probe_for_pinecone"))
            metric = "cosine"
            cloud = os.getenv("PINECONE_CLOUD", "aws")
            region = os.getenv("PINECONE_REGION", "us-east-1")
            if ServerlessSpec is not None:
                pc.create_index(
                    name=self.index_name,
                    dimension=dim,
                    metric=metric,
                    spec=ServerlessSpec(cloud=cloud, region=region),
                )
            else:
                # êµ¬ SDK í˜¸í™˜ (serverless spec ì—†ì´)
                pc.create_index(name=self.index_name, dimension=dim, metric=metric)

        embed_impl = getattr(embeddings, "_impl", embeddings)
        self.vs = PineconeVectorStore.from_documents(
            docs, embed_impl, index_name=self.index_name
        )
        return self

    def as_retriever(self, **kwargs):
        return self.vs.as_retriever(**kwargs)

    def persist(self):
        pass  # ì›ê²© ê´€ë¦¬í˜•


class OracleVectorVS(VectorStoreProvider):
    """Oracle Vector Search ì—°ë™ Placeholder.
    - ì‹¤ì œ êµ¬í˜„ì€ Oracle 23ai í…Œì´ë¸”+HNSW ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆ, upsert, ê²€ìƒ‰(SQL) ì–´ëŒ‘í„° í•„ìš”.
    - ì—¬ê¸°ì„œëŠ” ì„¤ê³„ í¬ì¸íŠ¸ë§Œ ìœ ì§€(í•™ìŠµ/ë°œí‘œìš© í…œí”Œë¦¿).
    """
    def __init__(self):
        self.ready = False
    def from_documents(self, docs, embeddings):
        # TODO: docs â†’ (id, text, embedding) ë³€í™˜ í›„ Oracle í…Œì´ë¸” ì—…ì„œíŠ¸ + ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
        # ì˜ˆì‹œ) embedding = embeddings.embed_documents([d.page_content for d in docs])
        #       cx_Oracle/oracledb ì‚¬ìš©, VECTOR ì»¬ëŸ¼ê³¼ HNSW ì¸ë±ìŠ¤ êµ¬ì„±
        self.ready = True
        return self
    def as_retriever(self, **kwargs):
        if not self.ready:
            raise RuntimeError("OracleVectorVSê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # TODO: ì§ˆì˜ ì„ë² ë”© í›„ SQLë¡œ top-k ê²€ìƒ‰, ê²°ê³¼ë¥¼ langchain Documentë¡œ ì–´ëŒ‘íŠ¸
        raise NotImplementedError("Oracle Vector Search ì–´ëŒ‘í„° êµ¬í˜„ í•„ìš”")
    def persist(self):
        pass


def get_vectorstore(name: str) -> VectorStoreProvider:
    if name == "faiss":
        return FaissVS()
    if name == "chroma":
        return ChromaVS()
    if name == "pinecone":
        idx = os.getenv("PINECONE_INDEX_NAME", "my-index")
        return PineconeVS(index_name=idx)
    if name == "oracle":
        return OracleVectorVS()
    raise ValueError(f"Unknown vectorstore provider: {name}")


# ##################################################
# 7) LLM êµ¬í˜„ â€” OpenAI(ê¸°ë³¸)
# ##################################################
class LLMProvider:
    def __init__(self, model_name: str):
        self.model_name = model_name
        self._impl = None

    def get(self):
        if self._impl is None:
            try:
                from langchain_openai import ChatOpenAI
            except Exception as e:
                raise RuntimeError("OpenAI LLMì„ ìœ„í•´ 'langchain-openai'ê°€ í•„ìš”í•©ë‹ˆë‹¤.") from e
            _ensure("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”", bool(os.getenv("OPENAI_API_KEY")))
            self._impl = ChatOpenAI(model=self.model_name, temperature=0.2)
        return self._impl


def get_llm(name: str) -> LLMProvider:
    if name.startswith("openai:"):
        _, model = name.split(":", 1)
        return LLMProvider(model)
    raise ValueError(f"Unknown LLM provider: {name}")


# ##################################################
# 8) Chain Builder â€” ConversationalRetrievalChain
# ##################################################

def build_chain(vs_provider: VectorStoreProvider, llm_provider: LLMProvider):
    try:
        # LangChain 0.2+ ê²½ë¡œ ìš°ì„ 
        from langchain.chains import ConversationalRetrievalChain
        from langchain.memory import ConversationBufferMemory
    except Exception:
        # ì¼ë¶€ êµ¬ë²„ì „ í˜¸í™˜ (í•„ìš” ì‹œ ë‹¤ë¥¸ import)
        from langchain.chains import ConversationalRetrievalChain
        from langchain.memory import ConversationBufferMemory

    retriever = vs_provider.as_retriever(search_kwargs={"k": 5})
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    llm = llm_provider.get()

    chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)
    return chain


# ##################################################
# 9) Streamlit UI â€” ì—…ë¡œë“œ/ì„ íƒ/ë¹Œë“œ/ì§ˆì˜
# ##################################################
import streamlit as st


def _sidebar_config():
    st.sidebar.header("êµ¬ì„± ì„ íƒ (í•œ íŒŒì¼ ë‚´ êµì²´)")

    emb_label = st.sidebar.selectbox(
        "Embeddings",
        options=[x[0] for x in UI_CHOICES["Embeddings"]],
        index=0,
    )
    emb_key = dict(UI_CHOICES["Embeddings"])[emb_label]

    # VectorStore ì„ íƒì§€: chromadb / pinecone ìœ ë¬´ + sqlite ì§€ì› ì—¬ë¶€ì— ë”°ë¼ ë¼ë²¨ ì•ˆë‚´ ë° ìë™ ëŒ€ì²´
    vector_candidates = [("FAISS (in-memory)", "faiss")]

    chroma_available = is_pkg_available("chromadb")
    sqlite_ok = is_sqlite_supported()
    if chroma_available:
        if sqlite_ok:
            vector_candidates.append(("ChromaDB (local client)", "chroma"))
        else:
            vector_candidates.append(("ChromaDB (SQLite<3.35 â€” ìë™ìœ¼ë¡œ FAISS ì‚¬ìš©)", "chroma"))
    else:
        vector_candidates.append(("ChromaDB (ë¯¸ì„¤ì¹˜ â€” ìë™ìœ¼ë¡œ FAISS ì‚¬ìš©)", "chroma"))

    pinecone_pkgs = is_pkg_available("langchain_pinecone") and is_pkg_available("pinecone")
    if pinecone_pkgs:
        vector_candidates.append(("Pinecone (managed)", "pinecone"))
    else:
        vector_candidates.append(("Pinecone (ë¯¸ì„¤ì¹˜ â€” ìë™ìœ¼ë¡œ FAISS ì‚¬ìš©)", "pinecone"))

    vs_label = st.sidebar.selectbox(
        "VectorStore",
        options=[x[0] for x in vector_candidates],
        index=0,
    )
    vs_key_requested = dict(vector_candidates)[vs_label]
    vs_key_effective = vs_key_requested

    # Pinecone ì„¤ì • ì…ë ¥ (ì„ íƒ ì‹œ í‘œì‹œ)
    if vs_key_requested == "pinecone":
        pinecone_api = st.sidebar.text_input("PINECONE_API_KEY (Pinecone ì„ íƒ ì‹œ í•„ìˆ˜)", type="password")
        if pinecone_api:
            os.environ["PINECONE_API_KEY"] = pinecone_api
        pinecone_idx = st.sidebar.text_input("PINECONE_INDEX_NAME", value=os.getenv("PINECONE_INDEX_NAME", "my-index"))
        if pinecone_idx:
            os.environ["PINECONE_INDEX_NAME"] = pinecone_idx
        pinecone_cloud = st.sidebar.text_input("PINECONE_CLOUD (aws/gcp)", value=os.getenv("PINECONE_CLOUD", "aws"))
        if pinecone_cloud:
            os.environ["PINECONE_CLOUD"] = pinecone_cloud
        pinecone_region = st.sidebar.text_input("PINECONE_REGION", value=os.getenv("PINECONE_REGION", "us-east-1"))
        if pinecone_region:
            os.environ["PINECONE_REGION"] = pinecone_region

        if not pinecone_pkgs or not os.getenv("PINECONE_API_KEY"):
            st.sidebar.info("Pinecone íŒ¨í‚¤ì§€ ë˜ëŠ” API í‚¤ê°€ ì—†ì–´ **FAISS**ë¡œ ìë™ ëŒ€ì²´í•©ë‹ˆë‹¤. ì„¤ì¹˜: `pip install langchain-pinecone pinecone`.")
            vs_key_effective = "faiss"

    # Chroma ì‚¬ìš© ë¶ˆê°€ ì¡°ê±´ â†’ FAISSë¡œ ìë™ ëŒ€ì²´
    if vs_key_requested == "chroma" and (not chroma_available or not sqlite_ok):
        if not chroma_available:
            st.sidebar.info("ChromaDBê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ **FAISS**ë¡œ ìë™ ëŒ€ì²´í•©ë‹ˆë‹¤. ì„¤ì¹˜: `pip install chromadb`.")
        elif not sqlite_ok:
            st.sidebar.info("í˜„ì¬ sqlite3 ë²„ì „ì´ ë‚®ì•„ ChromaDB ì‚¬ìš©ì´ ì œí•œë©ë‹ˆë‹¤ (>= 3.35.0 í•„ìš”). **FAISS**ë¡œ ìë™ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        vs_key_effective = "faiss"

    sp_label = st.sidebar.selectbox(
        "Splitter",
        options=[x[0] for x in UI_CHOICES["Splitter"]],
        index=0,
    )
    sp_key = dict(UI_CHOICES["Splitter"])[sp_label]

    llm_label = st.sidebar.selectbox(
        "LLM",
        options=[x[0] for x in UI_CHOICES["LLM"]],
        index=0,
    )
    llm_key = dict(UI_CHOICES["LLM"])[llm_label]

    chunk_size = st.sidebar.slider("chunk_size", 200, 2000, DEFAULT_CONFIG["chunk_size"], step=50)
    chunk_overlap = st.sidebar.slider("chunk_overlap", 0, 400, DEFAULT_CONFIG["chunk_overlap"], step=20)

    # OpenAI API í‚¤ ì…ë ¥ (ì…ë ¥ ì‹œ ì¦‰ì‹œ í™˜ê²½ë³€ìˆ˜ì— ë°˜ì˜)
    api_key_input = st.sidebar.text_input("OPENAI_API_KEY (OpenAI ì‚¬ìš© ì‹œ í•„ìˆ˜)", type="password", help="OpenAI LLM/ì„ë² ë”© ì‚¬ìš© ì‹œ ì…ë ¥")
    if api_key_input:
        os.environ["OPENAI_API_KEY"] = api_key_input

    # === View ì „í™˜ ë²„íŠ¼ ===
    if "view" not in st.session_state:
        st.session_state["view"] = "rag"
    st.sidebar.divider()
    if st.sidebar.button("FAISS Dashboard"):
        st.session_state["view"] = "faiss"
    if st.sidebar.button("RAG Mode"):
        st.session_state["view"] = "rag"

    return {
        **DEFAULT_CONFIG,
        "embeddings": emb_key,
        "vectorstore": vs_key_effective,
        "requested_vectorstore": vs_key_requested,
        "splitter": sp_key,
        "llm": llm_key,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "view": st.session_state["view"],
    }


def render_faiss_dashboard(cfg):
    st.header("ğŸ“Š FAISS Dashboard")
    st.caption(
        "ì´ í™”ë©´ì€ ë²¡í„° ì¸ë±ìŠ¤(FAISS)ì˜ ìƒíƒœ, êµ¬ì„±, ì„±ëŠ¥, ê´€ë¦¬ ê¸°ëŠ¥ì„ í•œ ë²ˆì— ë³´ì—¬ì¤ë‹ˆë‹¤. "
        "ë¬¸ì„œ ì—…ë¡œë“œâ†’ì¸ë±ì‹± í›„ ì—¬ê¸°ì„œ í˜„í™©ì„ í™•ì¸í•˜ì„¸ìš”.")

    vs_provider = st.session_state.get("_vs_provider")
    if not vs_provider:
        st.info("ì•„ì§ VectorStoreê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì¸ë±ìŠ¤ë¥¼ ë¹Œë“œí•˜ì„¸ìš”.")
        return
    if not is_faiss_backed(vs_provider):
        st.info("í˜„ì¬ VectorStoreê°€ FAISS ê¸°ë°˜ì´ ì•„ë‹™ë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ FAISSë¥¼ ì„ íƒí•˜ê±°ë‚˜, Chromaê°€ ë‚´ë¶€ì ìœ¼ë¡œ FAISSë¡œ í´ë°±ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return

    vsv = getattr(vs_provider, "vs", None)
    index = getattr(vsv, "index", None) if vsv else None
    if index is None:
        st.warning("FAISS ì¸ë±ìŠ¤ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì¸ë±ìŠ¤ë¥¼ ë¹Œë“œí•˜ì„¸ìš”.")
        return

    # --- ìš”ì•½ ë©”íŠ¸ë¦­ ---
    ntotal = getattr(index, "ntotal", 0)
    dim = getattr(index, "d", None)
    index_type = type(index).__name__
    metric_guess = "L2" if "L2" in index_type.upper() else ("IP" if "IP" in index_type.upper() else "?")
    mem_mb = (ntotal * (dim or 0) * 4) / (1024 * 1024) if dim else None

    c1, c2, c3, c4 = st.columns(4)
    with c1:
        st.metric("Vectors (ntotal)", ntotal)
        st.caption("ì¸ë±ìŠ¤ì— ì €ì¥ëœ ë²¡í„°(=ì²­í¬) ê°œìˆ˜. ë§ì„ìˆ˜ë¡ ê²€ìƒ‰ í›„ë³´ê°€ ëŠ˜ì–´ ì •í™•ë„ì— ìœ ë¦¬í•˜ì§€ë§Œ, ê²€ìƒ‰/ë©”ëª¨ë¦¬ ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤.")
    with c2:
        st.metric("Dimension (d)", dim if dim is not None else "-")
        st.caption("ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜. ì‚¬ìš©í•œ ì„ë² ë”© ëª¨ë¸ì— ì˜í•´ ê²°ì •ë©ë‹ˆë‹¤. (ì˜ˆ: all-MiniLM-L6-v2 â†’ 384)")
    with c3:
        st.metric("Metric", metric_guess)
        st.caption("FAISS ì¸ë±ìŠ¤ì˜ ê±°ë¦¬/ìœ ì‚¬ë„ ì§€í‘œ. IndexFlatL2ëŠ” L2(ìœ í´ë¦¬ë“œ ê±°ë¦¬), Inner ProductëŠ” IPë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
    with c4:
        st.metric("Est. Memory (MB)", f"{mem_mb:.2f}" if mem_mb is not None else "-")
        st.caption("ëŒ€ëµì ì¸ ë²¡í„° ì €ì¥ ìš©ëŸ‰ ì¶”ì •ì¹˜(ntotalÃ—dimÃ—4byte). ì¸ë±ìŠ¤/ë©”íƒ€ ë“± ë¶€ê°€ ì˜¤ë²„í—¤ë“œëŠ” í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # --- êµ¬ì„± ì •ë³´ ---
    st.subheader("êµ¬ì„± ì •ë³´")
    st.table([
        {"Key": "VectorStore", "Value": "FAISS"},
        {"Key": "Index Type", "Value": index_type},
        {"Key": "Embeddings", "Value": cfg["embeddings"]},
        {"Key": "Splitter", "Value": cfg["splitter"]},
        {"Key": "Chunk Size", "Value": cfg["chunk_size"]},
        {"Key": "Chunk Overlap", "Value": cfg["chunk_overlap"]},
        {"Key": "LLM", "Value": cfg["llm"]},
    ])
    st.caption(
        "â€¢ VectorStore: ì‹¤ì œ ê²€ìƒ‰ ë°±ì—”ë“œ
"
        "â€¢ Index Type: ì¸ë±ìŠ¤ êµ¬ì¡°(ì˜ˆ: IndexFlatL2 â€” ì •í™•í•˜ì§€ë§Œ í° ë°ì´í„°ì—ì„œ ëŠë¦´ ìˆ˜ ìˆìŒ)
"
        "â€¢ Embeddings: ì„ë² ë”© ëª¨ë¸ (ì°¨ì›Â·ì„±ëŠ¥ ì§€í‘œ ì°¨ì´ì— ì˜í–¥)
"
        "â€¢ Splitter/Chunk Size/Overlap: ë¶„í•  ì „ëµê³¼ í¬ê¸° (ê²€ìƒ‰ í’ˆì§ˆÂ·ì¸ë±ìŠ¤ í¬ê¸°Â·ì†ë„ì— ì˜í–¥)
"
        "â€¢ LLM: ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ (ì‘ë‹µ ì†ë„Â·ë¹„ìš©Â·í’ˆì§ˆì— ì˜í–¥)")

    # --- ì„±ëŠ¥ ì •ë³´ ---
    st.subheader("ì„±ëŠ¥ ì •ë³´")
    perf = st.session_state.get("_perf", {})
    chunk_time = perf.get("chunk_time_s")
    index_time = perf.get("index_time_s")
    q_times = perf.get("query_times", [])

    cols = st.columns(2)
    with cols[0]:
        st.markdown("**ì¸ë±ì‹±**")
        st.write(f"- Chunking: {chunk_time:.3f}s" if isinstance(chunk_time, (int, float)) else "- Chunking: -")
        st.write(f"- FAISS.from_documents: {index_time:.3f}s" if isinstance(index_time, (int, float)) else "- FAISS.from_documents: -")
        st.caption("Chunkingì€ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” ì‹œê°„, from_documentsëŠ” ì„ë² ë”© ê³„ì‚° + ì¸ë±ìŠ¤ ë¹Œë“œë¥¼ í•©ì¹œ ì‹œê°„ì…ë‹ˆë‹¤.")
    with cols[1]:
        st.markdown("**ì§ˆì˜ ì§€ì—°ì‹œê°„(ìµœê·¼)**")
        if q_times:
            st.write(f"- count={len(q_times)}, avg={sum(q_times)/len(q_times):.3f}s, min={min(q_times):.3f}s, max={max(q_times):.3f}s")
            st.line_chart(q_times)
            st.caption("ì§ˆì˜ ë²„íŠ¼ì„ ëˆ„ë¥¸ ìˆœê°„ë¶€í„° ë‹µë³€ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ì˜ ì´ ì†Œìš” ì‹œê°„ì…ë‹ˆë‹¤. ê²€ìƒ‰ + LLM í˜¸ì¶œ/ìƒì„±ì„ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.")
        else:
            st.write("- ìˆ˜ì§‘ëœ ì§ˆì˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.caption("ì—¬ê¸°ì— ì‹œê°„ì´ ëˆ„ì ë˜ë„ë¡, ì˜¤ë¥¸ìª½ â€˜ëŒ€í™”â€™ ì„¹ì…˜ì—ì„œ ì§ˆë¬¸ì„ ì‹¤í–‰í•´ ë³´ì„¸ìš”.")

    with st.expander("ğŸ”§ íŠœë‹ íŒ/ì„¤ëª… ë”ë³´ê¸°"):
        st.markdown(
            "- **ëŠë¦° ê²½ìš°**: retriever `k`ë¥¼ 5â†’3ìœ¼ë¡œ ë‚®ì¶”ê±°ë‚˜, ë” ê°€ë²¼ìš´ LLMì„ ì„ íƒí•˜ì„¸ìš”. ì´ˆê¸° ëª‡ ë²ˆì€ ëª¨ë¸/ëŸ°íƒ€ì„ ì›Œë°ì—…ìœ¼ë¡œ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"
            "- **ëŒ€ìš©ëŸ‰ ë°ì´í„°**: IndexFlatL2 ëŒ€ì‹  IVF/IVFPQ/HNSW ê°™ì€ ANN ì¸ë±ìŠ¤ë¥¼ ê²€í† í•˜ì„¸ìš”(ì •í™•ë„ ì¼ë¶€ í¬ìƒ, ì†ë„/ë©”ëª¨ë¦¬ ì ˆê°).
"
            "- **ì •í™•ë„ í–¥ìƒ**: chunk_sizeë¥¼ ë¬¸ì„œ íŠ¹ì„±ì— ë§ì¶° ì¡°ì •í•˜ê³ , MMR/ë©”íƒ€ í•„í„°ë§/í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ë³‘í–‰í•˜ì„¸ìš”.
"
            "- **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**: ë²¡í„°ë¥¼ L2 ì •ê·œí™”í•˜ê³  Inner Product(IP)ë¡œ ê²€ìƒ‰í•˜ë©´ ì½”ì‚¬ì¸ê³¼ ë™ì¼ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # --- ê´€ë¦¬(ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°) ---
    st.subheader("ê´€ë¦¬")
    st.caption(
        "â€˜FAISS ì €ì¥â€™ì€ index.faiss(ì¸ë±ìŠ¤) + docstore.pkl + index_to_docstore_id.pklì„ ì§€ì • í´ë”ì— ê¸°ë¡í•©ë‹ˆë‹¤.
"
        "â€˜FAISS ë¶ˆëŸ¬ì˜¤ê¸°â€™ëŠ” í•´ë‹¹ íŒŒì¼ë“¤ì„ ì½ì–´ ì¸ë±ìŠ¤ë¥¼ ë³µì›í•˜ê³ , ì²´ì¸ì„ ìë™ìœ¼ë¡œ ì¬ë¹Œë“œí•©ë‹ˆë‹¤.")
    save_col, load_col = st.columns(2)
    with save_col:
        save_dir = st.text_input("ì €ì¥ í´ë”", value=st.session_state.get("_faiss_save_dir", "./faiss_store"), key="faiss_save_dir")
        if st.button("FAISS ì €ì¥"):
            st.session_state["_faiss_save_dir"] = save_dir
            try:
                if hasattr(vsv, "save_local"):
                    vsv.save_local(save_dir)
                else:
                    import faiss, pickle
                    os.makedirs(save_dir, exist_ok=True)
                    faiss.write_index(index, os.path.join(save_dir, "index.faiss"))
                    with open(os.path.join(save_dir, "docstore.pkl"), "wb") as f:
                        pickle.dump(vsv.docstore, f)
                    with open(os.path.join(save_dir, "index_to_docstore_id.pkl"), "wb") as f:
                        pickle.dump(vsv.index_to_docstore_id, f)
                st.success(f"ì €ì¥ ì™„ë£Œ: {save_dir}")
            except Exception as e:
                st.exception(e)
    with load_col:
        load_dir = st.text_input("ë¶ˆëŸ¬ì˜¤ê¸° í´ë”", value=st.session_state.get("_faiss_load_dir", "./faiss_store"), key="faiss_load_dir")
        if st.button("FAISS ë¶ˆëŸ¬ì˜¤ê¸°"):
            st.session_state["_faiss_load_dir"] = load_dir
            try:
                from langchain_community.vectorstores import FAISS as FAISSClass
                emb = get_embeddings(cfg["embeddings"])
                embed_impl = getattr(emb, "_impl", emb)
                loaded = FAISSClass.load_local(load_dir, embed_impl, allow_dangerous_deserialization=True)
                provider = FaissVS(); provider.vs = loaded
                st.session_state["_vs_provider"] = provider
                st.session_state["_chain"] = build_chain(provider, get_llm(cfg["llm"]))
                st.success("ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ ë° ì²´ì¸ ê°±ì‹  ì™„ë£Œ")
            except Exception as e:
                st.exception(e)

    # --- ë¬¸ì„œ/ì²­í¬ ë©”íƒ€ ---
    st.subheader("ë¬¸ì„œ/ì²­í¬ ë©”íƒ€")
    st.caption("ì—…ë¡œë“œ íŒŒì¼, ë¬¸ì„œ/ì²­í¬ ìˆ˜, ì‹¤ì œ ì‚¬ìš©ëœ VectorStoreë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Chroma ì„ íƒâ†’í™˜ê²½ ë¬¸ì œë¡œ FAISS í´ë°±ëœ ê²½ìš°ë„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    meta = st.session_state.get("_faiss_meta", {})
    if meta:
        st.json(meta)
    else:
        st.write("ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ì‹±ì„ í•œ ë²ˆ ìˆ˜í–‰í•´ ë³´ì„¸ìš”.")

def main():
    st.set_page_config(page_title="RAG Single-File Template", page_icon="ğŸ“š", layout="wide")
    st.title("ğŸ“š RAG Single-File Template â€” ëª¨ë“ˆ êµì²´í˜•")

    cfg = _sidebar_config()

    # ë·° ì „í™˜: FAISS Dashboard ëª¨ë“œ
    if cfg.get("view") == "faiss":
        render_faiss_dashboard(cfg)
        return

    # â˜… ì¶”ê°€: OpenAI ì„ íƒëëŠ”ë° í‚¤ê°€ ì—†ìœ¼ë©´ ì‚¬ì´ë“œë°” ê²½ê³ 
    if (cfg["llm"].startswith("openai:") or cfg["embeddings"] == "openai") and not os.getenv("OPENAI_API_KEY"):
        st.sidebar.warning("OpenAI ì‚¬ìš© ì‹œ OPENAI_API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    # VectorStore ëŒ€ì²´ ì•ˆë‚´ (ìš”ì²­ê°’ê³¼ ì‹¤ì œ ì‚¬ìš©ê°’ì´ ë‹¤ë¥¸ ê²½ìš°)
    if cfg.get("requested_vectorstore") != cfg["vectorstore"]:
        st.sidebar.warning(
            f"ìš”ì²­í•œ VectorStore '{cfg.get('requested_vectorstore')}'ì„(ë¥¼) ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ "
            f"**{cfg['vectorstore'].upper()}** ë¡œ ìë™ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤. í•„ìš”í•œ íŒ¨í‚¤ì§€/í‚¤/í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    st.markdown("""
**RAG-Corpus:**


Loader â†’ Splitter(Seperator|tokenizer) â†’ (Chunk â†’ Embedding) â†’ (Vector Store â†’ Vector Index)


**Query-Serving:**


Query â†’ Query Embedding â†’ Retriever (Vector Search:Similarity|MMR|MetaFiltering) â†’ Prompt â†’ LLM (í˜¸ì¶œ|ì¶”ë¡ |ì‘ë‹µìƒì„±) â†’ Answer
    """)

    uploaded_files = st.file_uploader("ë¬¸ì„œ ì—…ë¡œë“œ (PDF/DOCX/PPT/TXT)", type=["pdf", "docx", "doc", "pptx", "ppt", "txt"], accept_multiple_files=True)

    build_col, chat_col = st.columns([1, 2])

    with build_col:
        st.subheader("1) ì¸ë±ìŠ¤ ë¹Œë“œ")
        if st.button("ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘", use_container_width=True):
            if not uploaded_files:
                st.error("ìµœì†Œ 1ê°œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            else:
                tmp_paths = []
                for uf in uploaded_files:
                    p = Path(st.secrets.get("_tmp_dir", ".")) / uf.name
                    with open(p, "wb") as f:
                        f.write(uf.getbuffer())
                    tmp_paths.append(str(p))

                try:
                    docs = load_documents(tmp_paths)
                    splitter = get_splitter(cfg["splitter"], cfg["chunk_size"], cfg["chunk_overlap"])
                    # ì„±ëŠ¥ ì¸¡ì •: split
                    t_split0 = time.perf_counter()
                    splits = split_documents(docs, splitter)
                    t_split1 = time.perf_counter()

                    # ì„±ëŠ¥ ì¸¡ì •: ë²¡í„° ì¸ë±ìŠ¤ ë¹Œë“œ
                    emb = get_embeddings(cfg["embeddings"])
                    t_index0 = time.perf_counter()
                    vs = get_vectorstore(cfg["vectorstore"]).from_documents(splits, emb)
                    t_index1 = time.perf_counter()

                    # ìƒíƒœ ì €ì¥
                    st.session_state["_vs_provider"] = vs
                    used_name = effective_vs_name(vs)
                    st.session_state["_faiss_meta"] = {
                        "files": [uf.name for uf in uploaded_files],
                        "num_docs": len(docs),
                        "num_chunks": len(splits),
                        "vectorstore_used": used_name,
                    }
                    perf = st.session_state.get("_perf", {})
                    perf["chunk_time_s"] = t_split1 - t_split0
                    perf["index_time_s"] = t_index1 - t_index0
                    perf.setdefault("query_times", [])
                    st.session_state["_perf"] = perf

                    st.session_state["_chain"] = build_chain(vs, get_llm(cfg["llm"]))
                    st.success("ì¸ë±ì‹± ë° ì²´ì¸ ì¤€ë¹„ ì™„ë£Œ")
                except Exception as e:
                    st.exception(e)

    with chat_col:
        st.subheader("2) ëŒ€í™”")
        q = st.text_input("ì§ˆë¬¸ ì…ë ¥")
        if st.button("ì§ˆì˜", use_container_width=True):
            chain = st.session_state.get("_chain")
            if not chain:
                st.warning("ë¨¼ì € ë¬¸ì„œ ì¸ë±ì‹±ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
            else:
                if not q or not q.strip():
                    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
                else:
                    t0 = time.perf_counter()
                    try:
                        res = chain.invoke({"question": q})  # langchain 0.2+ invoke
                    except Exception:
                        # ì¼ë¶€ ë²„ì „ì—ì„œëŠ” __call__ ì‚¬ìš©
                        res = chain({"question": q})
                    t1 = time.perf_counter()

                    # ì§ˆì˜ ì„±ëŠ¥ ëˆ„ì 
                    perf = st.session_state.get("_perf", {})
                    q_times = perf.setdefault("query_times", [])
                    q_times.append(t1 - t0)
                    # ìµœê·¼ 50ê°œë§Œ ìœ ì§€
                    if len(q_times) > 50:
                        q_times[:] = q_times[-50:]
                    st.session_state["_perf"] = perf

                    answer = res.get("answer") or res.get("result")
                    st.write(answer)


if __name__ == "__main__":
    main()
