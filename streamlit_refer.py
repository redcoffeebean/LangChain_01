import os
import io
import tempfile
from pathlib import Path
from typing import List

import streamlit as st
from loguru import logger

# ë³€ê²½: LangChain íŒ¨í‚¤ì§€ ë¶„ë¦¬ë¡œ ì¸í•œ import ê²½ë¡œ ìˆ˜ì •
from langchain_openai import ChatOpenAI  # ê¸°ì¡´: from langchain.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings  # ê¸°ì¡´: from langchain.embeddings import ...
from langchain_community.vectorstores import FAISS  # ê¸°ì¡´: from langchain.vectorstores import FAISS
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredPowerPointLoader,
)

# ë³€ê²½: text splittersê°€ ë³„ë„ íŒ¨í‚¤ì§€ë¡œ ì´ë™ë¨
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# -----------------------------
# ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ í•¨ìˆ˜
# -----------------------------
def _persist_upload(file) -> Path:
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ í´ë”ì— ì €ì¥ í›„ ê²½ë¡œ ë°˜í™˜"""
    suffix = Path(file.name).suffix
    tmp_dir = Path(tempfile.mkdtemp(prefix="st_docs_"))
    out_path = tmp_dir / file.name
    out_path.write_bytes(file.getbuffer())
    logger.info(f"ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ ê²½ë¡œ: {out_path}")
    return out_path

# -----------------------------
# ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜
# -----------------------------
def _load_document(path: Path):
    ext = path.suffix.lower()
    if ext == ".pdf":
        return PyPDFLoader(str(path))
    if ext == ".docx":
        return Docx2txtLoader(str(path))
    if ext in (".ppt", ".pptx"):
        return UnstructuredPowerPointLoader(str(path))
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")

# -----------------------------
# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
# -----------------------------
def build_vectorstore(doc_paths: List[Path]):
    """HF ì„ë² ë”©ì„ ì´ìš©í•˜ì—¬ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
    docs = []
    for p in doc_paths:
        loader = _load_document(p)
        docs.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=120)
    splits = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore

# -----------------------------
# ì²´ì¸ êµ¬ì„±
# -----------------------------
def get_chain(vectorstore, openai_api_key: str):
    # ë³€ê²½: ChatOpenAI ì„í¬íŠ¸ ê²½ë¡œ ë° ëª¨ë¸ëª… ìµœì‹ í™”
    llm = ChatOpenAI(
        openai_api_key=openai_api_key,
        model="gpt-4o-mini",
        temperature=0,
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
    )

    # ë³€ê²½: as_retriever() ë¶ˆí•„ìš”/ì˜¤íƒ€ íŒŒë¼ë¯¸í„° ì œê±°
    retriever = vectorstore.as_retriever(search_type="mmr")

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        memory=memory,
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=True,
    )
    return chain

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="RAG Chat (Patched)", page_icon="ğŸ§©")

st.title("RAG Chat with Your Files âœ¨")

with st.sidebar:
    st.subheader("ğŸ”‘ API & ì„¤ì •")

    # ê¸°ë³¸ê°’: secretsì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
    default_key = st.secrets.get("OPENAI_API_KEY", "") if hasattr(st, "secrets") else ""
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        value=default_key,
        help="Secretsì— OPENAI_API_KEYë¥¼ ì €ì¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤"
    )

    uploaded_files = st.file_uploader(
        "ë¬¸ì„œ ì—…ë¡œë“œ (PDF/DOCX/PPTX)",
        type=["pdf", "docx", "pptx"],  # ë³€ê²½: pptx ì§€ì› ì¶”ê°€
        accept_multiple_files=True,
    )

    build_btn = st.button("ì¸ë±ìŠ¤ ìƒì„±/ì¬ìƒì„±")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chain" not in st.session_state:
    st.session_state.chain = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ì¸ë±ìŠ¤ ë¹Œë“œ
if build_btn:
    if not openai_api_key:
        st.error("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not uploaded_files:
        st.warning("ìµœì†Œ 1ê°œ ì´ìƒì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        with st.spinner("ë¬¸ì„œ ì¸ë±ì‹± ì¤‘â€¦"):
            try:
                doc_paths = [_persist_upload(f) for f in uploaded_files]
                vs = build_vectorstore(doc_paths)
                st.session_state.vectorstore = vs
                st.session_state.chain = get_chain(vs, openai_api_key)
                st.success("ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
            except Exception as e:
                logger.exception("ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                st.error(f"ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")

# ì±„íŒ… ì…ë ¥
st.divider()
st.subheader("ğŸ’¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸í•˜ê¸°")
user_q = st.text_input("ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆ: ë¬¸ì„œ í•µì‹¬ ìš”ì•½ ì•Œë ¤ì¤˜")
ask = st.button("ì§ˆë¬¸í•˜ê¸°")

# QA ì‹¤í–‰
if ask:
    if not openai_api_key:
        st.error("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif st.session_state.chain is None:
        st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    elif not user_q.strip():
        st.info("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘â€¦"):
            try:
                result = st.session_state.chain({"question": user_q})
                answer = result.get("answer", "(ë‹µë³€ ì—†ìŒ)")
                sources = result.get("source_documents", [])

                st.session_state.chat_history.append(("user", user_q))
                st.session_state.chat_history.append(("assistant", answer))

                st.markdown("### ğŸ§  ë‹µë³€")
                st.write(answer)

                if sources:
                    st.markdown("### ğŸ“ ì°¸ê³  ë¬¸ì„œ")
                    with st.expander("ì°¸ê³  ë¬¸ì„œì™€ ì›ë¬¸ ì¼ë¶€ ë³´ê¸°"):
                        for i, doc in enumerate(sources, 1):
                            src = doc.metadata.get("source", f"source_{i}")
                            st.markdown(f"**{i}.** {src}")
                            preview = (doc.page_content or "").strip()
                            if len(preview) > 600:
                                preview = preview[:600] + " â€¦"
                            st.code(preview)
            except Exception as e:
                logger.exception("ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨")
                st.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
if st.session_state.chat_history:
    st.divider()
    st.subheader("ğŸ—‚ï¸ í˜„ì¬ ì„¸ì…˜ ëŒ€í™” ê¸°ë¡")
    for role, msg in st.session_state.chat_history[-10:]:
        if role == "user":
            st.markdown(f"**You:** {msg}")
        else:
            st.markdown(f"**Assistant:** {msg}")
