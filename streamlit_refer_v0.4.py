"""
streamlit_refer.py â€” RAG(Chat + Retrieval) ì•± (OpenAI LLM + HuggingFace ì„ë² ë”©)

[êµ¬ì„± ìš”ì•½]
- LLM: OpenAI Chat (gpt-4o-mini; ì €ë ´/ë¹ ë¦„)
- Embeddings: HuggingFace (paraphrase-MiniLM-L6-v2)
- ë¬¸ì„œ ë¡œë”: PDF, DOCX, PPTX
- ë²¡í„°ìŠ¤í† ì–´: FAISS
- ì²´ì¸: ConversationalRetrievalChain (ëŒ€í™”í˜• RAG)
- í´ë°±: ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ LLM-onlyë¡œ ê°„ë‹¨ ë‹µë³€ + "RAG: OFF" ì•ˆë‚´

[ì™œ ì´ë ‡ê²Œ?]
- ë¹„ìš©/ìœ ì—°ì„±: LLMì€ OpenAI, ì„ë² ë”©ì€ HFë¡œ ë¶„ë¦¬ â†’ ë¹„ìš©Â·ì„±ëŠ¥ ê· í˜•
- ìµœì‹  LangChain íŒ¨í‚¤ì§€ ë¶„ë¦¬ ë°˜ì˜: langchain-openai / langchain-huggingface / langchain-community
- ì´ˆê¸° ë¡œë“œ/ì¸ë±ì‹± ì„±ëŠ¥: ì„ë² ë”©/ìŠ¤í”Œë¦¬í„° ìºì‹œ(@st.cache_resource)
- ì•ˆì •ì„±: LLM í˜¸ì¶œì— timeout/retries, FAISS ë¯¸ì„¤ì¹˜ ì•ˆë‚´ ì²˜ë¦¬

[requirements.txt ì˜ˆì‹œ]
streamlit>=1.32
langchain>=0.2.0
langchain-community>=0.2.0
langchain-openai>=0.1.0
langchain-text-splitters>=0.2.0
langchain-huggingface>=0.1.0
sentence-transformers
torch
pypdf
docx2txt
unstructured
unstructured[pptx]
tiktoken
loguru
faiss-cpu   # â† Streamlit Cloud/ì¼ë°˜ CPU í™˜ê²½
"""

import os
import io
import tempfile
from pathlib import Path
from typing import List

import streamlit as st
from loguru import logger

# =========================
# LangChain ê´€ë ¨ ì„í¬íŠ¸
# =========================
# (ì¤‘ìš”) ChatOpenAIëŠ” langchain_openaiì—ì„œ ì œê³µ
from langchain_openai import ChatOpenAI

# (ì¤‘ìš”) í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”©ì€ langchain_huggingfaceì—ì„œ ì œê³µ
#  - ì˜ˆì „ì˜ langchain_community.embeddings.HuggingFaceEmbeddingsëŠ” 0.2.2ì—ì„œ Deprecated
from langchain_huggingface import HuggingFaceEmbeddings

# ì»¤ë®¤ë‹ˆí‹° ë²¡í„°ìŠ¤í† ì–´/ë¡œë” (FAISS ì„í¬íŠ¸ëŠ” í™˜ê²½ì— ë”°ë¼ ì‹¤íŒ¨ ê°€ëŠ¥ â†’ ì•ˆì „ ê°€ë“œ)
try:
    from langchain_community.vectorstores import FAISS
except Exception as _e:  # ImportError í¬í•¨
    FAISS = None
    _faiss_import_err = _e

from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredPowerPointLoader,
)

# í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°(ë³„ë„ íŒ¨í‚¤ì§€)
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ì²´ì¸/ë©”ëª¨ë¦¬
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# (LLM-only í´ë°±ìš©) ë©”ì‹œì§€ íƒ€ì…
from langchain.schema import SystemMessage, HumanMessage


# =========================
# Streamlit ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="RAG Chatbot (OpenAI + HF)", page_icon="ğŸ¤–")
st.title("RAG Chatbot âœ¨")
st.caption("ë¬¸ì„œê°€ ì—…ë¡œë“œëœ ê²½ìš°ëŠ” RAGë¡œ ë‹µë³€í•˜ê³ , ì—…ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°ëŠ” LLMì—ì„œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")


# =========================
# ìœ í‹¸/í—¬í¼
# =========================
def _persist_upload(file) -> Path:
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ í´ë”ì— ì €ì¥ í›„ ê²½ë¡œ ë°˜í™˜ (Streamlit ì—…ë¡œë“œ ê°ì²´ â†’ ì‹¤ì œ íŒŒì¼)
    - PDF, DOCX, PPTX ëª¨ë‘ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    """
    tmp_dir = Path(tempfile.mkdtemp(prefix="st_docs_"))
    out_path = tmp_dir / file.name
    out_path.write_bytes(file.getbuffer())
    logger.info(f"ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ ê²½ë¡œ: {out_path}")
    return out_path


def _load_document(path: Path):
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ë¬¸ì„œ ë¡œë” ë°˜í™˜"""
    ext = path.suffix.lower()
    if ext == ".pdf":
        return PyPDFLoader(str(path))
    if ext == ".docx":
        return Docx2txtLoader(str(path))
    if ext in (".ppt", ".pptx"):
        return UnstructuredPowerPointLoader(str(path))
    raise ValueError(f"ğŸ˜– ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")


# =========================
# ìºì‹œ: ì„ë² ë”©/ìŠ¤í”Œë¦¬í„°
# =========================
@st.cache_resource(show_spinner=False)
def get_hf_embeddings():
    """í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹œ)
    - ìµœì´ˆ 1íšŒë§Œ ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ì´í›„ ì„¸ì…˜ì—ì„œëŠ” ì¬ì‚¬ìš©
    - normalize_embeddings=True ëŠ” FAISS ê²€ìƒ‰ ì•ˆì •ì„±ì— ë„ì›€
    - Streamlit Cloudì—ì„œ ëª¨ë¸ ìºì‹œ ë””ë ‰í„°ë¦¬ë¥¼ /tmp/hf ë¡œ ì§€ì •
    """
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
        cache_folder="/tmp/hf",
    )


@st.cache_resource(show_spinner=False)
def get_splitter():
    """í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°(ìºì‹œ)
    - chunk_size/overlap íŠœë‹: ì²­í¬ ìˆ˜ ê°ì†Œ â†’ ì¸ë±ì‹± ì†ë„ í–¥ìƒ
    - ë¬¸ì„œ ì„±ê²©ì— ë”°ë¼ 1800/80 â†’ 2000/60 ë“±ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥
    """
    return RecursiveCharacterTextSplitter(chunk_size=1800, chunk_overlap=80)


# =========================
# ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ
# =========================
def build_vectorstore(doc_paths: List[Path]):
    """ì—…ë¡œë“œ ë¬¸ì„œë“¤ì„ ë¡œë“œâ†’ì²­í¬â†’ì„ë² ë”©â†’FAISS ì¸ë±ìŠ¤ ìƒì„±
    - PDFê°€ ìŠ¤ìº” ì´ë¯¸ì§€ì¼ ê²½ìš° íŒŒì‹±ì´ ëŠë¦´ ìˆ˜ ìˆìŒ(ê°€ëŠ¥í•˜ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF ê¶Œì¥)
    - FAISS ë¯¸ì„¤ì¹˜ ì‹œ ì¹œì ˆí•œ ì—ëŸ¬ ì•ˆë‚´
    """
    if FAISS is None:
        raise RuntimeError(
            f"ğŸ˜– FAISS ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì›ì¸: {repr(_faiss_import_err)})\n"
            "CPU í™˜ê²½ì—ì„œëŠ” requirements.txtì— 'faiss-cpu'ë¥¼ ì¶”ê°€í•´ ì£¼ì„¸ìš”.\n"
            "GPU(CUDA) í™˜ê²½ì—ì„œëŠ” 'faiss-gpu'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )

    docs = []
    for p in doc_paths:
        loader = _load_document(p)
        docs.extend(loader.load())

    splitter = get_splitter()
    splits = splitter.split_documents(docs)

    embeddings = get_hf_embeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore


# =========================
# ì²´ì¸ êµ¬ì„± (LLM + Retriever + ë©”ëª¨ë¦¬)
# =========================
def get_chain(vectorstore, openai_api_key: str):
    """ConversationalRetrievalChain êµ¬ì„±
    - LLMì€ OpenAI (gpt-4o-mini) ì‚¬ìš©
    - retrieverëŠ” FAISS.as_retriever(search_type="mmr") ì‚¬ìš©
    - ë©”ëª¨ë¦¬ëŠ” ëŒ€í™” ê¸°ë¡ì„ LLMì— ì œê³µ (ë‹µë³€ í’ˆì§ˆ í–¥ìƒ)
    """
    llm = ChatOpenAI(
        openai_api_key=openai_api_key,
        model="gpt-4o-mini",  # ì €ë ´ & ë¹ ë¦„ (í•„ìš” ì‹œ gpt-4o ë¡œ êµì²´ ê°€ëŠ¥)
        temperature=0,
        max_retries=3,  # ê°„ë‹¨í•œ ì¬ì‹œë„ (429 ë“± ë ˆì´íŠ¸ë¦¬ë°‹ ëŒ€ë¹„)
        timeout=20,     # ë„ˆë¬´ ì˜¤ë˜ ê¸°ë‹¤ë¦¬ì§€ ì•Šë„ë¡ íƒ€ì„ì•„ì›ƒ
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
    )

    retriever = vectorstore.as_retriever(search_type="mmr")

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",             # ê°„ë‹¨í•˜ê²Œ ë¬¸ì„œ ìŠ¤í„°í•‘ ë°©ì‹ ì‚¬ìš©(í•„ìš”ì‹œ map_reduce ë“± ë³€ê²½ ê°€ëŠ¥)
        memory=memory,
        get_chat_history=lambda h: h,   # ëŒ€í™” ë‚´ì—­ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
        return_source_documents=True,   # ì†ŒìŠ¤ ë¬¸ì„œ ë°˜í™˜(ê·¼ê±° í‘œì‹œìš©)
        verbose=True,                   # ë””ë²„ê¹… ë¡œê·¸
    )
    return chain


# =========================
# LLM ë‹¨ë…(ë¹„ RAG) ì‘ë‹µ í—¬í¼
# =========================
def answer_without_rag(question: str, openai_api_key: str) -> str:
    """ë¬¸ì„œ ì¸ë±ìŠ¤ê°€ ì—†ì„ ë•Œ, LLMë§Œìœ¼ë¡œ ê°„ê²°í•œ ë‹µë³€ì„ ìƒì„±
    - 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ì§§ê³  í•µì‹¬ë§Œ
    - RAGê°€ ì•„ë‹˜ì„ UIì—ì„œ ë³„ë„ ì•ˆë‚´
    """
    llm = ChatOpenAI(
        openai_api_key=openai_api_key,
        model="gpt-4o-mini",
        temperature=0,
        max_retries=3,
        timeout=20,
    )
    sys = SystemMessage(content="ë„ˆëŠ” ê°„ê²°í•œ ì¡°ìˆ˜ë‹¤. ëª¨ë“  ë‹µë³€ì€ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ë§í•´ë¼.")
    user = HumanMessage(content=question)
    resp = llm.invoke([sys, user])  # Chat ëª¨ë¸ì˜ invoke ì‚¬ìš©
    return getattr(resp, "content", str(resp))


# =========================
# ì‚¬ì´ë“œë°”(UI): API í‚¤/ë¬¸ì„œ ì—…ë¡œë“œ/ì¸ë±ìŠ¤ ë²„íŠ¼
# =========================
with st.sidebar:
    st.subheader("ğŸ”‘ OpenAI API Key")

    # ê¸°ë³¸ê°’: Streamlit Secretsì— OPENAI_API_KEYê°€ ìˆë‹¤ë©´ ìë™ ì‚¬ìš©
    default_key = st.secrets.get("OPENAI_API_KEY", "") if hasattr(st, "secrets") else ""
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        value=default_key,
        help="Streamlit Cloudì˜ Secretsì— OPENAI_API_KEYë¥¼ ë“±ë¡í•˜ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.",
    )

    uploaded_files = st.file_uploader(
        "ë¬¸ì„œ ì—…ë¡œë“œ (PDF/DOCX/PPTX)",
        type=["pdf", "docx", "pptx"],
        accept_multiple_files=True,
    )

    build_btn = st.button("ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±")


# =========================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =========================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chain" not in st.session_state:
    st.session_state.chain = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []


# =========================
# ì¸ë±ìŠ¤ ë¹Œë“œ ì‹¤í–‰
# =========================
if build_btn:
    if not openai_api_key:
        st.error("ğŸ”‘ OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not uploaded_files:
        st.warning("ìµœì†Œ 1ê°œ ì´ìƒì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        with st.spinner("ë¬¸ì„œ ë²¡í„° ì¸ë±ì‹± ì¤‘â€¦ (ìµœì´ˆì—ëŠ” ëª¨ë¸ ë¡œë”© ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            try:
                doc_paths = [_persist_upload(f) for f in uploaded_files]
                vs = build_vectorstore(doc_paths)
                st.session_state.vectorstore = vs
                st.session_state.chain = get_chain(vs, openai_api_key)
                st.success("âœ… Vector Index ìƒì„± ì™„ë£Œ! (RAG ê°€ëŠ¥)")
            except Exception as e:
                logger.exception("Vector Index ì‹¤íŒ¨")
                st.error(f"ğŸ˜– ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")


# =========================
# ì§ˆì˜ UI
# =========================
st.divider()
st.subheader("ğŸ’¬ ë¬¸ì„œ ê¸°ë°˜ ìì—°ì–´ ì§ˆë¬¸")
user_q = st.text_input("ì§ˆë¬¸ ì…ë ¥:", placeholder="ì˜ˆ: ì—…ë¡œë“œí•œ ë¬¸ì„œì˜ í•µì‹¬ë§Œ ê°„ë‹¨íˆ ì•Œë ¤ì£¼ì„¸ìš”")
ask = st.button("ì§ˆë¬¸í•˜ê¸°")


# =========================
# QA ì‹¤í–‰ (RAG ON/OFF í´ë°± í¬í•¨)
# =========================
if ask:
    if not openai_api_key:
        st.error("ğŸ”‘ OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not user_q.strip():
        st.info("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        # 1) ì¸ë±ìŠ¤/ì²´ì¸ ì¤€ë¹„ ì—¬ë¶€ í™•ì¸
        if st.session_state.chain is None:
            # ğŸ” í´ë°±: ë¬¸ì„œ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë¯€ë¡œ LLM ë‹¨ë… ê°„ë‹¨ ë‹µë³€
            with st.spinner("LLM ë‹µë³€ ìƒì„± ì¤‘â€¦ (RAG OFF)"):
                try:
                    answer = answer_without_rag(user_q, openai_api_key)
                    st.session_state.chat_history.append(("user", user_q))
                    st.session_state.chat_history.append(("assistant", answer))

                    st.markdown("### ğŸ§  ë‹µë³€  `RAG: OFF`")
                    st.write(answer)
                    st.info("RAG ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤. ì—…ë¡œë“œí•œ ë¬¸ì„œ/ì¸ë±ìŠ¤ê°€ ì—†ì–´ ì¼ë°˜ LLMìœ¼ë¡œ ê°„ë‹¨ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.exception("LLM-only ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨")
                    st.error(f"ğŸ˜– ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨(LLM-only): {e}")
        else:
            # âœ… RAG ê²½ë¡œ
            with st.spinner("RAG ì‘ë‹µ ìƒì„± ì¤‘â€¦ (RAG ON)"):
                try:
                    result = st.session_state.chain({"question": user_q})
                    answer = result.get("answer", "(ë‹µë³€ ì—†ìŒ)")
                    sources = result.get("source_documents", [])

                    st.session_state.chat_history.append(("user", user_q))
                    st.session_state.chat_history.append(("assistant", answer))

                    st.markdown("### ğŸ§  ë‹µë³€  `RAG: ON`")
                    st.write(answer)

                    # ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
                    if sources:
                        st.markdown("### ğŸ’¡ ì°¸ê³  ë¬¸ì„œ")
                        with st.expander("ì°¸ê³  ë¬¸ì„œ ìœ„ì¹˜ ë° ì›ë¬¸ ì¼ë¶€ ë³´ê¸°"):
                            for i, doc in enumerate(sources, 1):
                                src = doc.metadata.get("source", f"source_{i}")
                                st.markdown(f"**{i}.** {src}")
                                preview = (doc.page_content or "").strip()
                                if len(preview) > 600:
                                    preview = preview[:600] + " â€¦"
                                st.code(preview)
                    else:
                        st.info("í•´ë‹¹ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í•„ìš” ì‹œ ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•´ ë³´ì„¸ìš”.)")
                except Exception as e:
                    logger.exception("ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨(RAG)")
                    st.error(f"ğŸ˜– ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨(RAG): {e}")


# =========================
# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
# =========================
if st.session_state.chat_history:
    st.divider()
    st.subheader("ğŸ—‚ï¸ ì„¸ì…˜ ì•„ì¹´ì´ë¸Œ")
    for role, msg in st.session_state.chat_history[-10:]:
        if role == "user":
            st.markdown(f"**You:** {msg}")
        else:
            st.markdown(f"**Assistant:** {msg}")
